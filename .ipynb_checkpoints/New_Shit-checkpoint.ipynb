{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def eval_tree_based_model_max_depth(clf, max_depth, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function evaluates the given classifier (either a decision tree or random forest) at all of the \n",
    "    maximum tree depth parameters in the vector max_depth, using the given training and testing\n",
    "    data. It returns two vector, with the training and testing classification errors.\n",
    "    \n",
    "    Inputs:\n",
    "        clf: either a decision tree or random forest classifier object\n",
    "        max_depth: a (T, ) vector of all the max_depth stopping condition parameters \n",
    "                            to test, where T is the number of parameters to test\n",
    "        X_train: (N, D) matrix of training samples.\n",
    "        y_train: (N, ) vector of training labels.\n",
    "        X_test: (N, D) matrix of test samples\n",
    "        y_test: (N, ) vector of test labels\n",
    "    Output:\n",
    "        train_err: (T, ) vector of classification errors on the training data\n",
    "        test_err: (T, ) vector of classification errors on the test data\n",
    "    \"\"\"\n",
    "    training_errors = []\n",
    "    test_errors = []\n",
    "    # evaluates the tree classifier for the desired number of max_tree_depth values values \n",
    "    for i in range(len(max_depth)):\n",
    "        print(max_depth[i])\n",
    "        clf.set_params(min_samples_leaf = max_depth[i])\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_train_predict = clf.predict_proba(X_train)[:, 1]\n",
    "        y_test_predict = clf.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        training_errors.append(roc_auc_score(y_train, y_train_predict))\n",
    "        test_errors.append(roc_auc_score( y_test, y_test_predict))\n",
    "    return np.array(training_errors), np.array(test_errors)\n",
    "\n",
    "def classification_err(y, real_y):\n",
    "    \"\"\"\n",
    "    This function returns the classification error between two equally-sized vectors of \n",
    "    labels; this is the fraction of samples for which the labels differ.\n",
    "    \n",
    "    Inputs:\n",
    "        y: (N, ) shaped array of predicted labels\n",
    "        real_y: (N, ) shaped array of true labels\n",
    "    Output:\n",
    "        Scalar classification error\n",
    "    \"\"\"\n",
    "    return sum(y != real_y)/len(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(open(\"train_2008.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "test_data = np.loadtxt(open(\"test_2008.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "X = data[:, 3:382]\n",
    "y = data[:, 382]\n",
    "print(y)\n",
    "X_test = test_data[:, 3:382]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 43  36   8   3 338 196 333 191  34 198 322 330  52 337 230  10 234 336\n",
      "  25 233  26  38 332 231  27 324 189 260  47 249 327 321  61 235 320 236\n",
      "  32 326  44  13  28 319 341 226 194 323  12  14 335   4  59  49 363  56\n",
      "  33  55  29  50  15  62  45  94  24 229  54  71 186 354  92  16 156  18\n",
      " 175  40 232  60 360  83   0  57  51  80 353 228 187 176 184 357 364 160\n",
      "  39  30  69  70 167 334  48 179 329 213 361 276 352  67 163 288   5  17\n",
      "  65  74 343 181 318 210 246  88  19 172 173  75 174 223 180 162 356   7\n",
      " 270 251  41 351  90  98 102 225  35 224 159 269  20 362 114 331 158 134\n",
      " 261  97 204 272 142 209  86 349 157 151   1 242 328 227 275 296 316 355\n",
      " 166 271 315  89 100 298  84 350 274 206 307 103 263 262 348 311  99   6\n",
      " 308 265 317  31 203 268 183 266 101 168 325  72  37  78 313 273 201 300\n",
      " 154 219 237 264 202 290 254  96 221 310 309 306 314 305 164 289 220 312\n",
      " 205 132 182 299  87 358 292 212 238 301 291 293  91 282  46 295 165 359\n",
      " 294 115 283 297 286 222 245 127 281  63 152 129 267 133 147 208  82  93\n",
      " 284 285  21 116 131 146 150   2  81 287 170  79 145 218  95  58 153  85\n",
      " 136 130 253  68 344 161  77 244 135 367 252 185 117 366 155 257 342 140\n",
      " 105 256  76 369 104 241  66 304 365 192 368 303 339 188 190 370 138 177\n",
      " 149 109 110 195 302 345 340 106 215 197 169 214  23 277 118 255  22 113\n",
      " 250 258 259 126 112 278 279 280 247 111  11   9 346 347 108 107 248 217\n",
      " 243 123  64 137 139 125 141 143 144 148 124  53 171 178 122 119 121  73\n",
      " 120  42 199 200 207 211 216 128 239 240 193]\n",
      "371\n",
      "[ 43  36   8 333 191  34 198 322 330  52 337 230  10 234 336  25 233  26\n",
      "  38 332 231  27 324 189 260  47 249 327 321  61 235 320 236  32 326  44\n",
      "  13  28 319 341 226 194 323  12  14 335   4  59  49 363  56  33  55  29\n",
      "  50  15  62  45  94  24 229  54  71 186 354  92  16 156  18 175  40 232\n",
      "  60 360  83   0  57  51  80 353 228 187 176 184 357 364 160  39  30  69\n",
      "  70 167 334  48 179 329 213 361 276 352  67 163 288   5  17  65  74 343\n",
      " 181 318 210 246  88  19 172 173  75 174 223 180 162 356   7 270 251  41\n",
      " 351  90  98 102 225  35 224 159 269  20 362 114 331 158 134 261  97 204\n",
      " 272 142 209  86 349 157 151   1 242 328 227 275 296 316 355 166 271 315\n",
      "  89 100 298  84 350 274 206 307 103 263 262 348 311  99   6 308 265 317\n",
      "  31 203 268 183 266 101 168 325  72  37  78 313 273 201 300 154 219 237\n",
      " 264 202 290 254  96 221 310 309 306 314 305 164 289 220 312 205 132 182\n",
      " 299  87 358 292 212 238 301 291 293  91 282  46 295 165 359 294 115 283\n",
      " 297 286 222 245 127 281  63 152 129 267 133 147 208  82  93 284 285  21\n",
      " 116 131 146 150   2  81 287 170  79 145 218  95  58 153  85 136 130 253\n",
      "  68 344 161  77 244 135 367 252 185 117 366 155 257 342 140 105 256  76\n",
      " 369 104 241  66 304 365 192 368 303 339 188 190 370 138 177 149 109 110\n",
      " 195 302 345 340 106 215 197 169 214  23 277 118 255  22 113 250 258 259\n",
      " 126 112 278 279 280 247 111  11   9 346 347 108 107 248 217 243 123  64\n",
      " 137 139 125 141 143 144 148 124  53 171 178 122 119 121  73 120  42 199\n",
      " 200 207 211 216 128 239 240 193]\n"
     ]
    }
   ],
   "source": [
    "sum(X[:, 10]<0)\n",
    "print(i_love_indices)\n",
    "print(len(i_love_indices))\n",
    "to_delete = [1,2,3,4,5]\n",
    "print(np.delete(i_love_indices, to_delete[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64667, 379)\n",
      "(16000, 379)\n",
      "[9, 11, 126, 127, 128, 132, 133, 134]\n",
      "[9, 10, 11, 124, 125, 126, 127, 128, 131, 132, 133, 134, 215]\n",
      "(64667, 371)\n",
      "(16000, 371)\n",
      "(64667, 371)\n",
      "(16000, 366)\n"
     ]
    }
   ],
   "source": [
    "X[X < 0] = -1\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "bad_indices = []\n",
    "real_bad_indices = []\n",
    "for i in range(np.shape(X)[1]):\n",
    "    num_less = X[:,i] < 0\n",
    "    #bad_indices.append(float(sum(num_less))/float(len(num_less)))\n",
    "    if sum(num_less)/len(num_less) == 1:\n",
    "        real_bad_indices.append(i)\n",
    "        \n",
    "print(real_bad_indices)\n",
    "# print(bad_indices[45])\n",
    "\n",
    "real_bad_indices_2 = []\n",
    "for i in range(np.shape(X_test)[1]):\n",
    "    num_less = X_test[:,i] < 0\n",
    "    if sum(num_less)/len(num_less) == 1:\n",
    "        real_bad_indices_2.append(i)\n",
    "            \n",
    "print(real_bad_indices_2)  \n",
    "X = np.delete(X, real_bad_indices, axis = 1)\n",
    "X_test = np.delete(X_test, real_bad_indices,axis = 1)\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "imp = Imputer(missing_values=-1, strategy='mean')\n",
    "X = imp.fit_transform(X)\n",
    "imp = Imputer(missing_values=-1, strategy='mean')\n",
    "X_test = imp.fit_transform(X_test)\n",
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[ 42  35   8 333   3 194 187 328 192  33 317 325  37   9  51 225 229 228\n",
      "  24 331  25 327  26 332 226 244 322 255 185 319 321 230  31  46 231 318\n",
      "  12 221  43  60 315 336 190  11  27 316  13  58  48  32   4 358  55  53\n",
      " 224  14  28  49  61  93 330  70  54  23  44  91 355 152  59 171  17  50\n",
      "  15  56  39 314 227 180 182  79   0 348  82 223 349 172 352 359 183  69\n",
      " 324 156  38  66 163 313  29  73 356 241 175  68  74 329   5 170 208  47\n",
      " 159 205 218 347  18  16 169 283 176 168  87 338 177 271 357 158   7  40\n",
      " 220 246  97  89 204 155 138 222 101  19  64 265 346 270  34 200 219 344\n",
      " 326 113   1  88 264  83 351 102 343 266 237 312  96   6 306 147 311 258\n",
      " 153 295  99 320 162 261 154 285 256 302 202 310 164  30  85 267 130 263\n",
      "  95 291 350 293 345  71  36 269 197  98 303 100 260 232 179 305 323 294\n",
      "  77 214 259 308 160 296 249 215 284 304 309 128 198 207 199  86 257 150\n",
      " 268 287 286 307 300 353 201 354 216 301 233 178  90 289  45 288 290 114\n",
      " 282 123 277 292 217  62 161  20   2 240 129 262 278 115 148 146 125 279\n",
      " 281  84  81 141 127 126 166 143 203  78  80 280 276 157  94  92 131 142\n",
      " 103 213  67  57 116 239 339 149  75 132 248 236 363 247  65 104 361 251\n",
      " 117 297 133 250 136  63 134  76 173 365 362 135 252 151 334 299 165 337\n",
      " 275 340 186 298 364 145 110 209 188 191 243 242 245  21  22 238 112 111\n",
      " 253 109 108 107 234  10 360 106 254 272 273 105 274 235 139 118 335 137\n",
      " 144  72 167 124 122 174 181  52 184 121 189 212 120  41 119 341 193 342\n",
      " 195 140 206 210 211 196]\n",
      "366\n"
     ]
    }
   ],
   "source": [
    "print(len(real_bad_indices_2))\n",
    "\n",
    "print(i_love_indices)\n",
    "print(len(i_love_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits = 5, shuffle = True) \n",
    "\n",
    "# all_predictions = np.zeros(5)\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_val = X[train_index], X[test_index]\n",
    "#     y_train, y_val = y[train_index], y[test_index]\n",
    "    \n",
    "#     #fprint(np.shape(y_test))\n",
    "#     scaler = StandardScaler()\n",
    "#     # Fit on training set only.\n",
    "#     scaler.fit(X_train)\n",
    "#     # Apply transform to both the training set and the test set.\n",
    "#     X_train = scaler.transform(X_train)\n",
    "#     X_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "\n",
    "# for slices in [250,300]:\n",
    "#     training_errors = []\n",
    "#     test_errors = []\n",
    "#     max_depth = np.arange(15,200,15)\n",
    "#     for est in max_depth:\n",
    "        #print(slices, est)\n",
    "X = data[:, 3:382]\n",
    "y = data[:, 382]\n",
    "print('Loading Data')\n",
    "X[X < 0] = -1\n",
    "X = np.delete(X, real_bad_indices_2, axis = 1)\n",
    "imp = Imputer(missing_values=-1, strategy='mean')\n",
    "X = imp.fit_transform(X)\n",
    "X = np.delete(X, i_love_indices[len(i_love_indices) - 250:], axis=1)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components = 75)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "     #X_val = pca.transform(X_val)\n",
    "\n",
    "print('PCA done')\n",
    "#clf = RandomForestClassifier(n_estimators = 190, min_samples_leaf = est)\n",
    "clf = svm.SVC(probability = True)\n",
    "clf.fit(X_train, y_train)\n",
    "print('SVM done')\n",
    "#     print(clf.classes_)\n",
    "#clf.fit(X, y)\n",
    "\n",
    "#     predictions = clf.predict_proba(X_val)[:, 1]\n",
    "#     print(sum(np.floor(2.0*predictions) != np.array(y_val))/len(predictions))\n",
    "#     print(roc_auc_score(y_val, predictions))\n",
    "#     importances = clf.feature_importances_\n",
    "#     indices = np.argsort(importances)[::-1]\n",
    "#     #Print the feature ranking\n",
    "#     print(\"Feature ranking:\")\n",
    "#     for f in range(X.shape[1]):\n",
    "#         print(\"%d. feature %d (%f)\" % (f, indices[f], importances[indices[f]]))\n",
    "#         #print(X_train[:, indices[f]])\n",
    "\n",
    "\n",
    "#print(1) \n",
    "#max_depth = np.arange(10, 200, 20)\n",
    "#train_err, test_err = eval_tree_based_model_max_depth(clf, max_depth, X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "y_train_predict = clf.predict_proba(X_train)[:, 1]\n",
    "y_test_predict = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "training_errors.append(roc_auc_score(y_train, y_train_predict))\n",
    "test_errors.append(roc_auc_score(y_test, y_test_predict))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(max_depth, test_errors, label='Testing error')\n",
    "plt.plot(max_depth, training_errors, label='Training error')\n",
    "plt.xlabel('Maximum Tree Depth')\n",
    "plt.ylabel('Classification error')\n",
    "plt.title('Decision Tree with Gini Impurity and Maximum Tree Depth')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "print('Test error minimized at max_depth =', np.max(test_errors), max_depth[np.argmax(test_errors)])\n",
    "print(test_errors)\n",
    "\n",
    "#results = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmYFNW9//H3Z4YdRWRRBJVFwbiA20jctyvu0aiJ1100ajRqDDHG9WqIEsWrouKWaFCM+y9qgomKyzW4I4MaFLeg0cii4oag4jAz398fVT30DLP0TE/PMMPn9Tz9dNWpU6fO6a6ub1Wd6ipFBGZmZk1V1NoVMDOzts2BxMzM8uJAYmZmeXEgMTOzvDiQmJlZXhxIzMwsL6t8IJH0iKRjc8i3RNKQlqjTykRScdr29evJM1fSrs24zCGSljR33tYm6VhJj7R2PWrT3N9hE5b/mKQjW2v5lpB0sqQnGjtfmwgkkt6X9K2kxZK+lPR82uC86x8R+0TE5BzyrRYR7+W7vGzpBjrzqkzbmBlfKX5UEVGRtv0/AJLukPSbfMqU1F/SHyXNT9v6nqRJkjZKl/leRKyWY/3qzdvaG8hsETE5IvYBkNRBUkga1Lq1alj6nYek/WqkT0zTj8p3GRGxZ0TcmW85zUnS/2T9HpdKqsga/2cLLP9kSeVZy3xP0i2SNmim8r8nqbw5ymoTgST1g4hYHRgIXAacDfyxdauUn3QDvVq6IfwPSRszaSv8qCR1aPlaNi9JfYEXgU7AjsDqwNbAc8AerVi1gmoH3907wDGZEUkdgR8BzbpztTKJiIuzfp+nAc9k/T43r5m/QN/xP9LlrwHslabNzOx0rSzaUiABICIWRcQU4L+BYyVtBiCps6QrJP1H0seSbpLUNTOfpAMlvSrpK0nvSto7Tf+HpBPS4Q0lTZO0SNKnku7Nmj8kbZgOryHpdkkLJX0g6YLM0ZGk0ZKeTevyhaR/S9qnKW2VdImkeyXdLWkxcJSkIknnpW34VNI9ktbMmmcHSS+mR26vStq5jrJPlPRg1vi/Jd2dNb5A0mbZe86SfpZ+7uele0gPZhW5laTX0s/ubkmd62jWmcBC4Jj0aCIi4ouI+GNEXJ8ue0NJVbdcSD/PsemR6GJJj0rqVVveBj7PE9Lv99r085kj6fuSfiLpw3S9OSor/x2Srpf0ZLrcpyStV9dy03qOzlrW0+myPgcuSNP+kWZ/On2fnX6Wh0h6K3tdSdfpLyQNr6UtvSU9nK6DX0h6SNKAXD6zdProdN39VNI5OXx8fwF2lbRGOr4fUEryXWbKHJp+Rp+n5f4pkz+d9rmkzdPx9SR9JmmnOj67xnxPVfNmzf+PdDiz/p6S/mYWS7oorc+LSrYHdysJjI0iqUt22cDrafpmkv4v/V7elPTDrHm6Sro6bcdHSo7q6vqtVEnPDPwrIk5IP/f/ySpzJ0nT08/qZUk7ZE17UdLFkmYq+W3en/UdPg1kTl0vkbTl8tmqPvt3JTW4g9fmAklGRLwEzAV2SpMuA4YBWwAbAgOACwEkjQRuB84CegI7A+/XUuzFwGPAmsC6wMQ6Fj+RZA9hCLALyZ7acVnTvw+8DfQBLgf+KEmNbyUABwF3pcu7FxhD8iPeOa3jEuBaSH6cwBTgIqAXcA7wgKTetZQ7DdhZifUAATuk5QwDOgKzs2eIiBvSOvwu3Ss7KGvyocAoks9ka+DoOtqzB/BgNP7ePEcAxwJrA92BXzZy/oztgRlAb+DPwH3A5iTrzHHA9ZK6ZeU/imQ96gO8Afypkct6E+gLjK8xLRPgN00/y/tJ1tHs00T7A+9HxGu1lF0E3AysT3KUvgy4pkaeWj+zNDBdl04fAPQH+jXQlm+Bv5N8z5Cs87fXyCPgkrSsTUjWhf8BiIh/AecDdyrZwbsNuDkinqljeY39nhoyimTbsENajxuAw0g+uy2z2tUU+5Os81tK6gE8TnK2pA/J5zRJ6U4ocBXJ73Y4sBHJNiuXQJ7tAdLtnpJTo38haVMv4ALgL8rauUzrcCTJd90JuDJN3xmoyDrKeiUrvZTks78OuKWhCrXZQJKaD/RKN9InAWMi4vOIWAz8jmRFAfgJMCkiHo+IyoiYFxFv1VLeMpIVq39ELI2IZ2tmkFSclntuRCyOiPdJvpjsDecHEXFzRFQAk4F1SH7MTfFsRDyU1vtb4GTgvLQNS4GxwI+VHBEdA0yJiKlp/keBfwJ71yw0It4BykhW6F2Ah4FP0xV+F+DpRm7sr46IjyLiM+BvJD/a2vQBPsqMSDo43fNZLOnhesr/Y7pH9g3w/+opvyFzIuJP6XdzL8mGeGxEfBcRmeVnX1TxUEQ8FxHfAeeRBN91clzWfyLixnRv8tsc8v8J+IGk7un40dQRuCJiYUQ8GBHfRsRXJOv7LjWy1fWZ/Rj4S4125bKjcztwTHpksz3JTkt2nd6JiCcjoiwiPgEmZNcpIm4kOYX7EslG78J6ltXY76kh49Pf6yyS4P5oRLwfEV8AU0mCSVONi4gv0+/4IOD1iLgz/d5nAA8Bhyg59fUT4Iw0/yKSHeDD6i66VvNJPj9IdhQeiIgn0t/8wyQ7PHtm5b81It6KiCUkO5mHN1D+2xFxe9b2a6CknvXN0NbP2w4APifZ4+tGcu4wM01AcTq8HsmGsiG/JjkqeUnSF8CVETGpRp4+JHvrH2SlfZDWJaNqQxkR36R1yqnzuBYf1hhfH3hIUmWN9LVIguDhkrKPFDoCj9ZR9jRgV2Az4ElgKckPf5d0WmN8lDX8DctX9Jo+IwmsAETEAyRHTSeTnHPPtfymfp4fZw1/S7JH9lmNtOyyqz7/iFgkaRHJHvyiHJZV87urV0R8KOkl4GBJfyfZGJxcW15JqwFXp3kyP/LVa2Sr6zPrT/V2LVFy+q0h00j2ps8F/hoR32UfaEvqR3J0vENalyKyTn2lbibZoz4+IsrqWVZjv6eG1Cyv5ni9G8oGZH/PA0l2Nr7MSusAfEHyuXckOZ2ZmSagsR3eme1eZnmHS/px1vSO6bJqq98HQLes01u1qbneQPJZf1lLXqANBxJJ25B8oM8Cn5KsDJtGxLxasn8INHilQ0R8BJyYlr8j8ISkpyNiTla2T1l+5PJGmrY+UNtym0PNo4K5wBERMb1mRkkfkux9nJJj2dNIDvk3Jtk7XAocQhJIrsixPo31JHCQpEuacHqrNayXGUh/fGuQ7BFm0rqle/yw4umh+tpX17TJJKe3ViM5KvyojnxnAYOBkRHxkaQSklNBuViQzgtUBaW6Av/yCkeEpDtJjmB2qiXLeOA7YHhEfC7pR2StR+lpnwkkp31+K+kv6RFBvr4m2ZHMaOg0XXPL/i4/BB6LiB/UzJT2w5QDG9QIio31QyBzSvBD4JaIOL2e/OtlDa8PfJPuFDXb59TmTm1J6iFpf+Ae4I6IeC0iKkn2dCZIWivNN0BS5iqHPwLHSfovJZ3VAyR9r5ayfyxp3XT0C5IVpNqef3q4dx8wTtLqkgaSnHu+owDNrc1NwO+U/q9D0lqSDkin/YlkIz1Kyf8/ukjaTVL/OsqaRtJnoXSD9TRwAMlGbFYd83xM404p1HQFydHT7Ur+A6J0A7PCVTAriR9I2i7tEL2E5MqdBSR7bR+RXABRLOkkkp2LnKTr0Wes+Fk+QNLHdhor9kFkW51kb/GLtA+svtNENf0/4MAa7co1qE8ARkXEc3XU6WtgUdrv9qsa0ycCz6Udxo8D1zeizvV5leTUUde0f+/4Ziq3Kf5C0lfy35I6SuokaVtJwyJiGTAJuEZSn0z/pKRRDRWarmMbSPo9MJLkO4Nkx+PH6batOP0M/qtGkBgtaVi6w/AbklOFAJ+QdLbX+R+xXLWlQPKQkiuXPiTpWLqK6h3cZwNzgBclfQU8QdKZlemYP47kR7CIZANa249+G2C6kj+4TSE5l1nb5Y2nk/xg3iM5IrqLZAVpCVeRnKp6Mv08niepN2l/zUEkHZwLSc5Hn0kd33NEvEFyFPJMOv4FyUUIz6bBuTa3AJsruSLlz42tfHrufFuSPbPngcXAy0AX4NTGltcC7iD50X4KjCC9BDY9mjqRZO/8U5JO4BWOEhtwEXBX2kd0cFru1yQbo/XT97pcRXJ09BnJ55jzHx3TfoIzSHaI5rE8KOYy72cR8WQdky8i2cgtIvn93J+ZkLZvd5Z/x2cA20n671zrXY8rSALhJyS/w5baqVtB+hvai2R7s4Dk6PUSktNNAL9I00pJPqdHSdaduuyabo++Ijma7wSUZPp40+3TISR9pZ+SnLo6g+q/+T8Bd5N815Uk24RMXS8n6RL4UlJT+x1R2zi7YNbyJN1B0un7mxZe7m+B9SNidEsu19ofSS8C10VEQYNrm+0jMWuP0tNUx5H8X8esTWhLp7bM2jVJp5CcjvxrRDzf2vUxy5VPbZmZWV58RGJmZnlpN30kffr0iUGDBrV2NczM2pSZM2d+GhF98ymj3QSSQYMGUVpa2trVMDNrUyR90HCu+vnUlpmZ5cWBxMzM8uJAYmZmeXEgMTOzvDiQmJlZXgoWSCRNkvSJpNfrmC4lj3OcI2mWpK2yph0r6V/p69hC1dHMzPJXyCOS26jlyXxZ9gGGpq+TgBsBlDx97SKSW2mPBC5S9cdGmpnZSqRg/yOJiKeVPE+4LgcCt6e3435RUk8ljzDdFXg8Ij4HkPQ4SUC6u1B15ZFz4KPaHottZtYG9BsO+1zWaotvzT6SAVR/BOTcNK2u9BVIOklSqaTShQtrPtHTzMxaQpv+Z3tE/AH4A0BJSUnT7z7ZipHczKyta80jknlUf5bwumlaXelmZrYSas1AMgU4Jr16a1tgUfos7KnAnpLWTDvZ90zTzMxsJVSwU1uS7ibpOO8jaS7JlVgdASLiJuBhYF+S56x/Q/r89Yj4XNLFwIy0qN9mOt7NzGzlU8irtg5vYHoAp9YxbRIwqRD1MjOz5uV/tpuZWV4cSMzMLC8OJGZmlhcHEjMzy4sDiZmZ5cWBxMzM8uJAYmZmeXEgMTOzvDiQmJlZXhxIzMwsLw4kZmaWFwcSMzPLiwOJmZnlxYHEzMzy4kBiZmZ5cSAxM7O8OJCYmVleHEjMzCwvDiRmZpYXBxIzM8uLA4mZmeXFgcTMzPLiQGJmZnlxIDEzs7w4kJiZWV4cSMzMLC8OJGZmlhcHEjMzy4sDiZmZ5cWBxMzM8uJAYmZmeXEgMTOzvDiQmJlZXhxIzMwsLw4kZmaWFwcSMzPLS0EDiaS9Jb0taY6kc2qZPlDSk5JmSfqHpHWzplVIejV9TSlkPc3MrOk6FKpgScXA9cAoYC4wQ9KUiHgjK9sVwO0RMVnS7sClwNHptG8jYotC1c/MzJpHIY9IRgJzIuK9iCgD7gEOrJFnE+D/0uGnapluZmYruUIGkgHAh1njc9O0bP8EDk6HDwJWl9Q7He8iqVTSi5J+WNsCJJ2U5ilduHBhc9bdzMxy1Nqd7b8CdpH0CrALMA+oSKcNjIgS4Ajgakkb1Jw5Iv4QESURUdK3b98Wq7SZmS1XsD4SkqCwXtb4umlalYiYT3pEImk14JCI+DKdNi99f0/SP4AtgXcLWF8zM2uCQh6RzACGShosqRNwGFDt6itJfSRl6nAuMClNX1NS50weYAcgu5PezMxWEgULJBFRDpwGTAXeBO6LiNmSfivpgDTbrsDbkt4B1gbGpekbA6WS/knSCX9Zjau9zMxsJaGIaO06NIuSkpIoLS1t7WqYmbUpkmam/dFN1tqd7WZm1sY5kJiZWV7qDSRKrFdfHjMzW7XVG0gi6UB5uIXqYmZmbVAup7ZelrRNwWtiZmZtUi5/SPw+cKSkD4CvAZEcrIwoaM3MzKxNyCWQ7FXwWpiZWZvV4KmtiPgA6An8IH31TNPMzMwaDiSSzgDuBNZKX3dIOr3QFTMzs7Yhl1NbPwG+HxFfA0gaD7wATCxkxczMrG3I5aotsfzW7qTDKkx1zMysrcnliORWYLqkB9PxHwJ/LFyVzMysLWkwkETEVenzQHZMk46LiFcKWiszM2sz6g0kkoqB2RHxPeDllqmSmZm1JQ3dIqWC5Hkh67dQfczMrI3JpY9kTWC2pJdI/tkOQEQcUPcsZma2qsglkPxPwWthZmZtVi59JL+JiN1aqD5mZtbG5NJHUilpjRaqj5mZtTG5nNpaArwm6XGq95H8vGC1MjOzNiOXQPJA+jIzM1tBLn9InCypK7B+RLzdAnUyM7M2JJe7//4AeBV4NB3fQtKUQlfMzMzahlxu2vgbYCTwJUBEvAoMKWCdzMysDcklkCyLiEU10ioLURkzM2t7culsny3pCKBY0lDg58Dzha2W2arp888/Z968eZSVlbV2Vawd6NSpEwMGDKBXr14FXU4ugeR04HzgO+AuYCpwSSErZbYq+vzzz/nwww/ZYIMN6NatG0VFuZwwMKtdZWUl33zzDe+88w7vvvsuW221FcXFxQVZVi7PbP8mIs6PiG3S1wURsbQgtTFbhc2bN48NNtiA1VZbzUHE8lZUVMRqq63GsGHDqKys5PHHH6eioqLhGZuyrIKUamaNVlZWRrdu3Vq7GtbOdOvWjQ4dOvDGG2/w/vvvF2QZDiRmKxEfiVhzy6xTxcXFLFpU87qpZlpGQUo1M7OViiTKy8sLUnaDne2S+gInAoOy80fE8QWpkZlZIxx22GEsWbKEv/3tb61dlVVWLldt/RV4BngCKExPjZm1WZLqnT5w4MBmOTd/yy23cNppp7F0afVrfX7/+98TEXmXb02XSyDpFhFnF7wmZtYmLViwoGr4+eef55BDDuHll19mnXXWASjYJacZa6yxcj/loqysjE6dOuWc3pCIoKKigg4dctl8t4xc+kj+JmnfgtfEzNqkfv36Vb0yf3zr27dvVVrfvn2BZMN5/vnnM3DgQLp27cpmm23GrbfeWq2sG264gY022oguXbrQu3dvdtttNz7++GMeffRRTjzxRL777jskIYmTTz4ZSE5t7b///lVlZMavv/561l9/fdZYYw0OPvhgPv3002rLuvzyy+nfvz/dunVjv/3249Zbb0XSCvmyRQRXXXUVw4YNo0uXLmy00UZcfvnl1S6r7devH2PHjuWkk06iV69ejBo1iqVLlyKJG2+8kUMPPZTVV1+dE088EYDZs2ez99570717d1ZffXV++MMfVjuCu+mmm1httdWYOnUqm2++OZ06deKZZ55pwjdVOLmEtDOA8ySVAcvStIiIHoWrlpkBjH1oNm/M/6rFl7tJ/x5c9INNm7XMY445hnfeeYdJkyYxZMgQXnjhBX7605/SqVMnjjzySJ577jl+8YtfMHnyZLbffnu++uornn8+uYnG7rvvzpVXXsl5551XtZGt71LpZ599lt69e/PII4/wxRdfcPjhh3Puuedy8803A3DXXXdxwQUXMGHCBPbcc0+mTZvGueee22Abzj33XO677z6uvvpqhg8fzuuvv85Pf/pTli1bxvnnn1+V78orr+Scc85h+vTp1YLMhRdeyMUXX8yll15KRLBkyRJGjRrF5ptvzrPPPkt5eTljxoxh3333ZdasWVVHHUuXLuXCCy9k4sSJDBgwgJ49ezb68y+kXG4jv3pTC5e0N3ANUAzcEhGX1Zg+EJgE9AU+B46KiLnptGOBC9Ksl0TE5KbWw8xa11tvvcW9997Le++9x+DBgwEYPHgwr7/+OhMnTuTII4/kP//5Dz169OCAAw6ge/fuAAwfPryqjB49kn3Xfv36Nbi87t27c8stt9CxY0cATjjhBG677baq6VdeeSXHHnssp556KgBDhw7l9ddf55prrqmzzEWLFjFhwgSmTp3KrrvuWtWGBQsWcOGFF1YLJDvttBPnnXde1XimX+fQQw+tOpICuP7661myZAl33313VXC45557GDJkCA888ACHHnooABUVFVx33XVss802Dba9NeR0kk3SAcDO6eg/IqLByyPS571fD4wC5gIzJE2JiDeysl0B3J4+82R34FLgaEm9gIuAEiCAmem8X+TaMLP2oLmPClrLjBkzgOqBAaC8vLwqaOy7776MGzeOQYMGMWrUKHbffXcOPvjgJt0natNNN60KIgD9+/fn448/rhp/8803+dnPflZtnu22267eQDJr1izKysrYb7/9ql1gUFFRwdKlS1m8eDGrr57sd48cObLWMmqmz549mxEjRlQ7wlh33XUZMmQIs2fPrkorLi5mq622qq/JrSqXy38vA7YB7kyTzpC0Q0Q0dBw4EpgTEe+l5dwDHAhkB5JNgF+mw08Bf0mH9wIej4jP03kfB/YG7m6wRWa20qmsrEQSM2bMqLaBh+V/mFtjjTV49dVXeeaZZ3jyySeZOHEiv/71r5k2bdoKAaghNTuxJVFZWblCWmPbADBlyhQGDhy4wvRMQKw5XFeexujSpUvBL1rIRy6d7fsCoyJiUkRMItmg75fDfAOAD7PG56Zp2f4JHJwOHwSsLql3jvMi6SRJpZJKFy5cmEOVzKw1lJSUEBHMmzePDTfcsNpryJDljzfq0KEDu+22G5dccgmvvPIKa665Jvfccw+QBIfmulfUxhtvzAsvvFAt7cUXX6x3nhEjRtCxY0f+/e9/r9CGDTfcsEl3Jdh0002ZNWsWX375ZVXa3Llzee+999hss80aXV5ryfX6sZ4kfRgAzXmt3a+A6ySNBp4G5tGI/6pExB+APwCUlJT4QnKzldSmm27KEUccwejRo7n88sv5/ve/z+LFiyktLWXRokWceeaZ/PnPf2b+/PnsuOOO9OnTh+nTpzN//nw22WQTIOmPKC8v5+GHH2bkyJF07dq1yXv4Z555Jscddxxbb701e+yxB08//XRVwKrrSGXNNdfkrLPO4le/+hXl5eXsvvvulJWVMWvWLGbPns24ceMaXY9jjz2WcePGcfjhh/O73/2uqrN9ww035KCDDmpS21pDLiH0UuAVSbdJmgzMBHL5xOYB62WNr5umVYmI+RFxcERsSXKreiLiy1zmNbO2ZfLkyZxyyin85je/YeONN2bUqFHceeedbLDBBkCyoX7ggQcYNWoUw4YN44ILLuCSSy7hyCOPBJIO7FNOOYVjjz2Wvn37cuaZZza5LkcccQQXX3wxY8eOZcSIEdx///1ccEFybU+XLl3qnG/cuHFcdtll3HDDDQwfPpydd96ZiRMnVl1A0FirrbYajz/+OJWVley4447svvvu9O7dm4cffnil+p9IQ5TLP0IlrUPSTwLwUkR8lMM8HYB3gP8iCQIzgCMiYnZWnj7A5xFRKWkcUBERF6ad7TOBTO/Sy8DWmT6T2pSUlERpaWmDbTFbWc2cOZOtt966tauxyjrvvPOYPHky8+a1v33WmTNn8txzz7Htttuu0OEvaWZElORTfp0hT9L3IuItSZmN+dz0vb+k/hHxcn0FR0S5pNNIHoRVDEyKiNmSfguURsQUYFfgUklBcmrr1HTezyVdTBJ8AH5bXxAxM2uMb775hhtuuIG99tqLrl278sQTT3DNNddw1llntXbV2qT6jp1+CZwEXFnLtAB2b6jwiHgYeLhG2oVZw38G/lzHvJNI/mNiZtasJPHYY48xfvx4lixZwuDBgxk7dixjxoxp7aq1SXUGkog4KR3cp+YTESXVfRLRzGwl17VrVx577LHWrka7kUtn+/M5ppmZ2Sqovj6SfiT/3egqaUsgc01cD8DPAzUzM6D+PpK9gNEkl95elZW+GDivthnMzGzVU18fyWRgsqRDIuL+FqyTmZm1Ibnc/fd+SfsBmwJdstJ/W8iKmZlZ29BgZ7ukm4D/Bk4n6Sf5MbDiHcvMzGyVlMtVW9tHxDHAFxExFtgOGFbYapmZWVuRSyD5Nn3/RlJ/kqckrlO4KpmZLffWW28hicbeAqlfv35cccUVBaqVZcvlrmB/k9QT+F+Se14FcEtBa2VmbUZDz/UYOHBgtWeQN9bQoUNZsGABffr0adR8r732WpPvDmyNk0tn+8Xp4P2S/gZ0iYhFha2WmbUVCxYsqBp+/vnnOeSQQ3j55ZdZZ53kxEVdD2QqKytb4QFUtSkuLs7p8bo19e3bt9HztKS62r9s2bIVHv6VT3ktIZfO9lPTIxIi4jugSNLPGpjNzFYR/fr1q3plHovbt2/fqrTMBr1fv36MHTuWk046iV69ejFq1CgArrjiCkaMGEH37t3p378/Rx11FJ988klV+TVPbWXGH3jgAfbZZx+6devGhhtuyF133bVCvbJPbfXr149x48Zx6qmn0rNnT/r168fZZ59d7cmJX3/9Nccffzw9evSgV69e/PznP+fMM89s8CFTX331FaeeeirrrLMO3bt3p6SkhIceemiFNtx7773sueeedOvWjXHjxvHoo48iialTp7LddtvRuXNn7rjjDgD++te/suWWW9K5c2fWXnttfv7zn/Ptt99WlXnYYYex//77c+WVVzJw4EC6dOlCLndzL4RcTm2dGBHXZ0Yi4gtJJwI3FK5aZgbAI+fAR6+1/HL7DYd9Lmv2Yq+88krOOeccpk+fXvW0Q0lcffXVDB48mPnz5zNmzBiOPvpopk6dWm9ZZ599NuPHj2fixIncdNNNjB49mu23355BgwbVu/zzzz+fGTNmMGPGDI4++mhGjBhR9cyTMWPGMHXqVO655x6GDBnCzTffzC233MJ6661XZ5mVlZXss88+dO3alfvvv5+1116bRx99lIMPPpinnnqKHXfcsSrvr3/9a8aPH8/vf/97JPHWW28ByYO2Lr/8cjbeeGM6d+5MaWkpBx10EGeddRZ33303c+bM4aSTTuLbb7/l5ptvripv2rRpdO/enYceeqjVggjkFkiKJSnSWkoqBlrn+MnM2rSddtqJ886rfmOM7AdUDR48mGuuuYbtt9+ezz77jN69e9dZ1pgxYzj44ORJ3b/73e+YOHEi06ZNqzeQ7LHHHlXLGzp0KDfffDNPPPEERx55JF988QW33nort912G/vuuy+QBJ4nn3yS8vLyOst87LHHePXVV/nkk0+q+mROPfVUnnvuOa677rpqgeS0007jsMMOqxrPBJKLLrqoapkAZ5xxBjvuuCPjx48H4Hvf+x4TJkyoeiDntnPMAAAVO0lEQVRX5lRf586due222+jatWud9WsJuQSSR4F7Jf0+Hf9pmmZmhVaAo4LWVPOhSgBPPPEE48eP56233uLLL7+sOtX0wQcf1BtItthii6rhTp060adPHz7++ON6l589D0D//v2r5nnnnXcoLy9n2223rZZnu+2245lnnqmzzBkzZvDtt9+y9tprV0svKytj+PDh1dJqa39t6bNnz64Kkhm77LILlZWVvPnmm1WBZPjw4a0eRCC3QHI2SfA4JR1/HF+1ZWZNUPMqqjlz5rD//vtzwgknMHbsWHr37s27777LfvvtR1lZWb1l1exYllStv6Op8zR0FVpNlZWVrLXWWjz77LMrTOvcuXO18bquImvq1WUry1VpuVy1VQncmL7MzJrN9OnTWbZsGVdffXXVM8qfe+65VqnLsGHD6NChAy+88AJDhgypSn/xxRfrna+kpIRPPvmEiGDo0KHNUpdNN92Up59+ulratGnTKCoqYuONN26WZTSn+m4jf19EHCrpNZL/jlQTESMKWjMza/eGDRtGZWUlEyZM4Ec/+hEvv/wyl156aavUZc011+S4447j7LPPplevXgwZMoRbbrmFf//73/V2tu+zzz7suOOOHHDAAYwfP57hw4fz2Wef8eyzz9KzZ09Gjx7d6LqcffbZjBw5knPOOYfjjjuOOXPm8Mtf/pLjjz++SZdCF1p9l//+In3fH/hBLS8zs7xss802XHXVVVxzzTVssskmTJw4kQkTJrRafSZMmMCoUaM49NBD2W677SgrK+OII46gS5e6HwpbVFTEI488wn777cfpp5/ORhttxP77789jjz1W7cimMUpKSnjwwQd59NFHGTFiBMcffzyHHHII1157bVObVlCq65IxSS9HxFaS/hQRR7dwvRqtpKQkGnsLBbOVycyZM9l6661buxpWw/bbb8/gwYO58847W7sqTTZz5kyee+45tt122xU69iXNjIiSfMqvr4+kk6QjgO0lHVxzYkQ8kM+CzcxWNq+88gqzZ8/m+9//PkuXLmXSpEm88MILjBs3rrWrtlKrL5CcDBwJ9GTFU1kBOJCYWbtz7bXXVv2/Y+ONN+bvf/87u+22WyvXauVW3xMSnwWelVQaEX9swTqZmbWKLbfckpdeeqm1q9Hm1HfV1u4R8X/AFz61ZWZmdanv1NYuwP9R+xVaPrVlVgCVlZUUFeXymCCz3DT0J83mUN+prYvS9+MKXgszo0ePHrz77rusv/76dOrUqdH/sDbLFhGUlZXx/vvvs3TpUiKiYDspDf6zXdIZwK3AYuBmYCvgnIh4rCA1MltFbbDBBnzwwQfMmjXLRyXWLCKCL7/8kk8++YSKigp69OhRkOXkcq+t4yPiGkl7Ab2Bo4E/AQ4kZs2oqKiIQYMGMX/+fKZPn07Pnj2b9IAjs2wVFRUsWrSIAQMGMHDgwIIsI5dAkjm+3he4PSJmy8fcZgUhie23356OHTvy5ptvsnjx4taukrVxnTp1YqONNmK33XZb4SaSzSWXQDJT0mPAYOBcSasDhe+9MVtFSWLkyJF13nLcbGWTSyD5CbAF8F5EfCOpF+AOeDMzA3J4ZjuwHfB2RHwp6SjgAmBRYatlZmZtRS6B5EbgG0mbA2cC7wK3F7RWZmbWZuQSSMrT57UfCFwXEdcDqxe2WmZm1lbk0keyWNK5wFHAzpKKAF+TaGZmQG5HJP8NfAf8JCI+AtYF/jeXwiXtLeltSXMknVPL9PUlPSXpFUmzJO2bpg+S9K2kV9PXTY1ok5mZtaBcntn+EXBV1vh/yKGPRFIxcD0wCpgLzJA0JSLeyMp2AXBfRNwoaRPgYWBQOu3diNgi14aYmVnraPCIRNK2kmZIWiKpTFKFpFyu2hoJzImI9yKiDLiHpJ8lWwCZ/+yvAcxvTOXNzKz15XJq6zrgcOBfQFfgBOCGHOYbAHyYNT43Tcv2G+AoSXNJjkZOz5o2OD3lNU3STrUtQNJJkkollS5cuDCHKpmZWXPL6c5wETEHKI6Iioi4Fdi7mZZ/OHBbRKxLcguWP6Wd+QuA9SNiS+CXwF2SVrjbWET8ISJKIqKkb9++zVQlMzNrjFyu2vpGUifgVUmXk2zkcwlA84D1ssbXTdOy/YQ0KEXEC5K6AH0i4hOSDn4iYqakd4FhQGkOyzUzsxaUS0A4GigGTgO+JgkOh+Qw3wxgqKTBaSA6DJhSI89/gP8CkLQx0AVYKKlv2lmPpCHAUOC9HJZpZmYtLJertj5IB78FxuZacESUSzoNmEoSiCaldw7+LVAaEVNI/il/s6QxJB3voyMiJO0M/FbSMpIbRJ4cEZ83qmVmZtYilPxpvZYJ0mskG/daRcSIQlWqKUpKSqK01Ge+zMwaQ9LMiCjJp4z6jkj2z6dgMzNbNdQXSDoCa0fEc9mJknYAPiporczMrM2or7P9auCrWtK/SqeZmZnVG0jWjojXaiamaYMKViMzM2tT6gskPeuZ1rW5K2JmZm1TfYGkVNKJNRMlnQDMLFyVzMysLamvs/0XwIOSjmR54CgBOgEHFbpiZmbWNtQZSCLiY2B7SbsBm6XJf4+I/2uRmpmZWZuQyz/bnwKeaoG6mJlZG5TT3X/NzMzq4kBiZmZ5cSAxM7O8OJCYmVleHEjMzCwvDiRmZpYXBxIzM8uLA4mZmeXFgcTMzPLiQGJmZnlxIDEzs7w4kJiZWV4cSMzMLC8OJGZmlhcHEjMzy4sDiZmZ5cWBxMzM8uJAYmZmeXEgMTOzvDiQmJlZXhxIzMwsLw4kZmaWFwcSMzPLiwOJmZnlxYHEzMzy4kBiZmZ5cSAxM7O8FDSQSNpb0tuS5kg6p5bp60t6StIrkmZJ2jdr2rnpfG9L2quQ9TQzs6brUKiCJRUD1wOjgLnADElTIuKNrGwXAPdFxI2SNgEeBgalw4cBmwL9gSckDYuIikLV18zMmqaQRyQjgTkR8V5ElAH3AAfWyBNAj3R4DWB+OnwgcE9EfBcR/wbmpOWZmdlKppCBZADwYdb43DQt22+AoyTNJTkaOb0R8yLpJEmlkkoXLlzYXPU2M7NGaO3O9sOB2yJiXWBf4E+Scq5TRPwhIkoioqRv374Fq6SZmdWtYH0kwDxgvazxddO0bD8B9gaIiBckdQH65DivmZmtBAp5RDIDGCppsKROJJ3nU2rk+Q/wXwCSNga6AAvTfIdJ6ixpMDAUeKmAdTUzsyYq2BFJRJRLOg2YChQDkyJitqTfAqURMQU4E7hZ0hiSjvfRERHAbEn3AW8A5cCpvmLLzGzlpGS73faVlJREaWlpa1fDzKxNkTQzIkryKaO1O9vNzKyNcyAxM7O8OJCYmVleHEjMzCwvDiRmZpYXBxIzM8uLA4mZmeXFgcTMzPLiQGJmZnlxIDEzs7wU8u6/bUJZeSVPvPkxnYqL6NQh61Vc4z07vbiIoiK1dtVtFRERLKsIviuv4LvyyuS1rIKyikqEKC6C4qIiiiWKiqC4SMlLWj5cJIq0PN3rb+NlvofyykqWVQQVlUF5RSXL0vfyyqC8IliWDlek+corgmWVlVRkzVteWUl5RaTzLC9vWSY9U15lWl5W3kxaReXy+gzu052LfrBpq302q3wg+WrpMn5258uNnq9jsWoNMh2Li+hcW0DqUFw13LlG/uwyOhfXNu/y8c4dkiBWma5kFZlXRFVa1bRa0ioj/QHUkpZ5ZaZV1EivqCN/RR15M2kdikSH4iI6FosORUmbOxaLDul4pw5Fy/MUiY7peMfiIjoUa3n+ouQ9SU/yZsrN5O1QlHxOHdLyOxYVVSuvuAkb0IhYvgEvr+C7ZVnD5ZXpePWN/HfllZSV55avtuk1523uW+JJVAWUDlnBpWYQKiqCDkVFFCkToIqSwJU1b1GNgFVVlkRx8fKyJIiAyohq70FQWZmOp593ZSPyZb8HyfTKSrLy1CwnzZPmI2s8yZfMV7WRT98rW/i2hB2Lk8+tY1GyPhcXLf/ddCxK1uXM+r/W6p1btnI1rPKBZI2uHXn0FztRlv54y8or+a6istr4sopKyiqW/7jLypePl9UY/y6Tv7ySpcsq+erb8hWml5VXVI239MrZWBJVG4sORcs3NrWl1bUXXBlBWXlltb2vZVl7Vtl7cS3Rno5VP8isANUh+XEC1Tfu6febr8xOQOeORXTuUFy1M9G5YzK8WucO9O6+fFp2vs5Z+ZJXMZ07JgE5ghV2GGoG9Zo7D9Wm1zovVFRWUpFutOvaYcgut6y8slpZ2dOXL4eqgFKk5F0kw1XjEkXZ09Px7HyIJE9RUVZZSvPUNm8SEJfnSdO1vC615cvseGQ23MmOSrLeZO+YZNahDlnvVWmZ/Fkb/8y616Fo+c5UtbSiTOBtO0eNq3wg6VhcxPf69Wg4Y4FkNqxJAKtYITDVFtwqI1bYaGfvWdZMK1Kywtbci1whrUaAaOmVefleYFBWUVntMH5ZxfIglJweqH7aYFn5ioGp6pRDVZ6sUwsVlSvkKatIAkZmQ1210c5xw15tPCvdp0KtvVvlA0lrS/ZWoGunYqBja1enVUlKT1NBV4pbuzpmliNftWVmZnlxIDEzs7w4kJiZWV4cSMzMLC8OJGZmlhcHEjMzy4sDiZmZ5cWBxMzM8qJo7pv4tBJJC4EP8iiiD/BpM1VnZeZ2ti9uZ/vSGu0cGBF98ymg3QSSfEkqjYiS1q5Hobmd7Yvb2b601Xb61JaZmeXFgcTMzPLiQLLcH1q7Ai3E7Wxf3M72pU22030kZmaWFx+RmJlZXhxIzMwsL6tEIJE0SdInkl7PSttC0ouSXpVUKmlkmi5J10qaI2mWpK1ar+aNI2k9SU9JekPSbElnpOm9JD0u6V/p+5ppeptsaz3t/F9Jb6VteVBSz6x5zk3b+bakvVqv9rmrq51Z08+UFJL6pOPt6vtMp52efqezJV2eld5uvs92sS2KiHb/AnYGtgJez0p7DNgnHd4X+EfW8CMkj4neFpje2vVvRDvXAbZKh1cH3gE2AS4HzknTzwHGt+W21tPOPYEOafr4rHZuAvwT6AwMBt4Filu7HU1tZzq+HjCV5E+4fdrp97kb8ATQOZ22Vnv8PtvDtmiVOCKJiKeBz2smA5mHta8BzE+HDwRuj8SLQE9J67RMTfMTEQsi4uV0eDHwJjCApE2T02yTgR+mw22yrXW1MyIei4jyNNuLwLrp8IHAPRHxXUT8G5gDjGzpejdWPd8nwATg1yTrcUa7+j6BU4DLIuK7dNon6Szt7fts89uiVSKQ1OEXwP9K+hC4Ajg3TR8AfJiVby7Lf7xthqRBwJbAdGDtiFiQTvoIWDsdbvNtrdHObMeT7M1BO2unpAOBeRHxzxrZ2lU7gWHATpKmS5omaZs0W3trZ5vfFq3KgeQUYExErAeMAf7YyvVpNpJWA+4HfhERX2VPi+SYuV1c811XOyWdD5QDd7ZW3ZpTdjtJ2nUecGGrVqoAavk+OwC9SE7rnAXcJ0mtWMVmUUs72/y2aFUOJMcCD6TD/4/lh8bzSM4/Z6ybprUJkjqSrKR3RkSmfR9nDonT98wpgjbb1jraiaTRwP7AkWnQhPbVzg1I+gX+Kel9kra8LKkf7audkOyBP5Ce2nkJqCS5qWF7a2eb3xatyoFkPrBLOrw78K90eApwTHrFxLbAoqzTQiu1dG/tj8CbEXFV1qQpJCsr6ftfs9LbXFvraqekvUn6DQ6IiG+yZpkCHCaps6TBwFDgpZasc1PU1s6IeC0i1oqIQRExiGRju1VEfEQ7+z6Bv5B0uCNpGNCJ5M647eb7TLX9bVFr9/a3xAu4G1gALCP54f0E2BGYSXL1x3Rg6zSvgOtJrgR5DShp7fo3op07kpy2mgW8mr72BXoDT5KsoE8AvdpyW+tp5xySc8qZtJuy5jk/befbpFfIrOyvutpZI8/7LL9qq719n52AO4DXgZeB3dvj99ketkW+RYqZmeVlVT61ZWZmzcCBxMzM8uJAYmZmeXEgMTOzvDiQmJlZXhxIbKWT3tH2jqzxDpIWSvpbE8s7QNI5zVfDRi37wfSurnMkLUqHX5W0fTMvZ4+0/FckvZPeUmTfPMobIumwrPETJF3dPLW19qZDa1fArBZfA5tJ6hoR3wKjyOMfvRExheTPXS0uIg4CkLQr8KuI2L+2fJI6xPIbTjbVUxHxw7S8rYAHJR0TEdOaUNYQ4DDgnjzrZKsAH5HYyuphYL90+HCSP5UCIGmkpBfSve/nJW2Upo+RNCkdHi7pdUndJI2WdF2afpukG9PnP7wnaVclz6t5U9JtWctYkjX8o8y0XOfPhaS5ki6T9ApwkKShkqZKminp6fTf3EhaW9IDSp5V8VL6L+d6RXKX2XHAafWVIekSSZPT9vxL0vFpEZcBu6VHTz9P09ZN6/cvSZc2pq3WzrX2PyL98qvmC1gCjAD+DHQh+QfwrsDf0uk9WP7ckT2A+9PhIuBp4CCgFNghTR8NXJcO30ayly2S23R/BQxP550JbJGpQ1Z9fgTc1pj5a2lTVf2z0uYCv8wafwrYIB3eAXgsHb4X2DYdHkTWc3Wy5t0D+EuNtBLgtfrKAC4h+dd4F2CttE5r1ywPOIHkzgg9gK4kdxDo39rril8rx8untmylFBGzlNxq+3CSo5NsawCTJQ0lueVEx3SeSiU3bZwF/D4inquj+IciIiS9BnwcEa8BSJpNspF9tYHq5Tt/tnvTeXuS3OX2fi2/wW3m97kHsFFW+ppZp/3qk32n3FrLSIf/EhFLgaWSnga2AZbWUt4Tkd5lWdJbwPosf3aGrcIcSGxlNoXk+Qy7ktwvLONikv6Ag9Jg84+saUNJjmj611Pud+l7ZdZwZjzzm8i+d1CXJsyfq6/TdwGfRsQWteQRMDIiyhpZ9pYkD0+qs4w0sNS8T1Jd903KbmsF3n5Yyn0ktjKbBIzN7PFnWYPlne+jM4mS1gCuJXm0cm9JP8pj2R9L2lhSEcmpsoKKiC+ABZIynfNFkjZPJz8BnJrJK6m2YFNNmuc8kpv+NVTGD9M76fYFdiI5LbiY5HGwZg1yILGVVkTMjYhra5l0OXBp2kmdvVc8Abg+It4hucPzZZLWauLizwH+BjxPcufolnAYcLKkfwKzSZ6rAkkA2EHSLElvACfWMf9u6QUIb5ME1J/F8iu26ivjdWAaSVsvioiPgVeAYkn/zOpsN6uV7/5rtgqTdAnJKTX/R8SazEckZmaWFx+RmJlZXnxEYmZmeXEgMTOzvDiQmJlZXhxIzMwsLw4kZmaWl/8PZHrNcztruz8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error minimized at max_depth = 0.7811935031645026 285\n",
      "[0.7790570432941638, 0.7785818265192835, 0.7808646139064485, 0.7798827898621545, 0.7797951339764745, 0.7797162280544273, 0.7807279894742938, 0.7801513199960762, 0.7807501143823371, 0.7794381510851845, 0.7800824140324667, 0.7790893869097356, 0.7805945056540857, 0.7799728519878338, 0.7802944912760206, 0.7811935031645026]\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(max_depth, test_errors[8:], label='Testing error')\n",
    "plt.plot(max_depth, training_errors[8:], label='Training error')\n",
    "plt.xlabel('Maximum Tree Depth')\n",
    "plt.ylabel('Classification error')\n",
    "plt.title('Decision Tree with Gini Impurity and Maximum Tree Depth')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "print('Test error minimized at max_depth =', np.max(test_errors[8:]), max_depth[np.argmax(test_errors[8:])])\n",
    "print(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "0. feature 42 (0.091376)\n",
      "1. feature 35 (0.043214)\n",
      "2. feature 8 (0.038293)\n",
      "3. feature 3 (0.032313)\n",
      "4. feature 333 (0.024999)\n",
      "5. feature 187 (0.024711)\n",
      "6. feature 194 (0.024011)\n",
      "7. feature 192 (0.020167)\n",
      "8. feature 328 (0.015910)\n",
      "9. feature 33 (0.015622)\n",
      "10. feature 317 (0.015297)\n",
      "11. feature 325 (0.014396)\n",
      "12. feature 37 (0.014310)\n",
      "13. feature 9 (0.014145)\n",
      "14. feature 331 (0.013797)\n",
      "15. feature 228 (0.013411)\n",
      "16. feature 229 (0.013410)\n",
      "17. feature 24 (0.013407)\n",
      "18. feature 225 (0.013049)\n",
      "19. feature 327 (0.013030)\n",
      "20. feature 25 (0.012847)\n",
      "21. feature 51 (0.012614)\n",
      "22. feature 332 (0.012068)\n",
      "23. feature 26 (0.011889)\n",
      "24. feature 231 (0.011803)\n",
      "25. feature 60 (0.011241)\n",
      "26. feature 226 (0.011149)\n",
      "27. feature 319 (0.010441)\n",
      "28. feature 255 (0.010276)\n",
      "29. feature 230 (0.009814)\n",
      "30. feature 185 (0.009467)\n",
      "31. feature 31 (0.008839)\n",
      "32. feature 322 (0.008807)\n",
      "33. feature 321 (0.008145)\n",
      "34. feature 43 (0.008006)\n",
      "35. feature 336 (0.007862)\n",
      "36. feature 316 (0.007798)\n",
      "37. feature 46 (0.007730)\n",
      "38. feature 221 (0.007616)\n",
      "39. feature 190 (0.007572)\n",
      "40. feature 4 (0.007510)\n",
      "41. feature 58 (0.007196)\n",
      "42. feature 11 (0.007115)\n",
      "43. feature 27 (0.006940)\n",
      "44. feature 12 (0.006870)\n",
      "45. feature 318 (0.006786)\n",
      "46. feature 315 (0.006781)\n",
      "47. feature 244 (0.006473)\n",
      "48. feature 13 (0.006393)\n",
      "49. feature 224 (0.006348)\n",
      "50. feature 358 (0.005982)\n",
      "51. feature 314 (0.005970)\n",
      "52. feature 48 (0.005672)\n",
      "53. feature 55 (0.005665)\n",
      "54. feature 32 (0.005435)\n",
      "55. feature 14 (0.005070)\n",
      "56. feature 44 (0.005068)\n",
      "57. feature 28 (0.004883)\n",
      "58. feature 54 (0.004833)\n",
      "59. feature 93 (0.004673)\n",
      "60. feature 23 (0.004559)\n",
      "61. feature 330 (0.004508)\n",
      "62. feature 61 (0.004435)\n",
      "63. feature 91 (0.004186)\n",
      "64. feature 53 (0.004175)\n",
      "65. feature 182 (0.003835)\n",
      "66. feature 59 (0.003700)\n",
      "67. feature 15 (0.003638)\n",
      "68. feature 171 (0.003630)\n",
      "69. feature 152 (0.003587)\n",
      "70. feature 39 (0.003557)\n",
      "71. feature 348 (0.003551)\n",
      "72. feature 227 (0.003503)\n",
      "73. feature 49 (0.003478)\n",
      "74. feature 17 (0.003434)\n",
      "75. feature 70 (0.003411)\n",
      "76. feature 352 (0.003358)\n",
      "77. feature 56 (0.003222)\n",
      "78. feature 355 (0.003180)\n",
      "79. feature 349 (0.003173)\n",
      "80. feature 79 (0.003170)\n",
      "81. feature 180 (0.003114)\n",
      "82. feature 38 (0.003091)\n",
      "83. feature 0 (0.003045)\n",
      "84. feature 50 (0.003020)\n",
      "85. feature 82 (0.003012)\n",
      "86. feature 183 (0.002773)\n",
      "87. feature 359 (0.002499)\n",
      "88. feature 172 (0.002464)\n",
      "89. feature 156 (0.002354)\n",
      "90. feature 69 (0.002325)\n",
      "91. feature 223 (0.002320)\n",
      "92. feature 324 (0.002180)\n",
      "93. feature 73 (0.002149)\n",
      "94. feature 163 (0.002112)\n",
      "95. feature 241 (0.002012)\n",
      "96. feature 29 (0.001975)\n",
      "97. feature 68 (0.001962)\n",
      "98. feature 329 (0.001829)\n",
      "99. feature 356 (0.001784)\n",
      "100. feature 208 (0.001589)\n",
      "101. feature 175 (0.001542)\n",
      "102. feature 47 (0.001516)\n",
      "103. feature 5 (0.001465)\n",
      "104. feature 351 (0.001440)\n",
      "105. feature 159 (0.001395)\n",
      "106. feature 347 (0.001339)\n",
      "107. feature 205 (0.001307)\n",
      "108. feature 16 (0.001274)\n",
      "109. feature 18 (0.001232)\n",
      "110. feature 313 (0.001217)\n",
      "111. feature 338 (0.001207)\n",
      "112. feature 170 (0.001190)\n",
      "113. feature 66 (0.001169)\n",
      "114. feature 169 (0.001162)\n",
      "115. feature 218 (0.001159)\n",
      "116. feature 346 (0.001157)\n",
      "117. feature 271 (0.001117)\n",
      "118. feature 177 (0.001092)\n",
      "119. feature 87 (0.001084)\n",
      "120. feature 219 (0.001083)\n",
      "121. feature 158 (0.001064)\n",
      "122. feature 40 (0.001062)\n",
      "123. feature 176 (0.001061)\n",
      "124. feature 97 (0.000994)\n",
      "125. feature 168 (0.000988)\n",
      "126. feature 220 (0.000985)\n",
      "127. feature 113 (0.000983)\n",
      "128. feature 357 (0.000953)\n",
      "129. feature 283 (0.000934)\n",
      "130. feature 246 (0.000902)\n",
      "131. feature 154 (0.000900)\n",
      "132. feature 101 (0.000886)\n",
      "133. feature 74 (0.000876)\n",
      "134. feature 7 (0.000855)\n",
      "135. feature 34 (0.000853)\n",
      "136. feature 89 (0.000850)\n",
      "137. feature 19 (0.000847)\n",
      "138. feature 138 (0.000825)\n",
      "139. feature 155 (0.000765)\n",
      "140. feature 291 (0.000756)\n",
      "141. feature 64 (0.000754)\n",
      "142. feature 162 (0.000750)\n",
      "143. feature 266 (0.000744)\n",
      "144. feature 350 (0.000736)\n",
      "145. feature 153 (0.000703)\n",
      "146. feature 326 (0.000688)\n",
      "147. feature 204 (0.000677)\n",
      "148. feature 222 (0.000672)\n",
      "149. feature 200 (0.000662)\n",
      "150. feature 264 (0.000649)\n",
      "151. feature 269 (0.000636)\n",
      "152. feature 312 (0.000626)\n",
      "153. feature 258 (0.000625)\n",
      "154. feature 130 (0.000621)\n",
      "155. feature 270 (0.000612)\n",
      "156. feature 344 (0.000590)\n",
      "157. feature 1 (0.000587)\n",
      "158. feature 257 (0.000582)\n",
      "159. feature 85 (0.000579)\n",
      "160. feature 83 (0.000553)\n",
      "161. feature 311 (0.000549)\n",
      "162. feature 6 (0.000526)\n",
      "163. feature 265 (0.000522)\n",
      "164. feature 237 (0.000520)\n",
      "165. feature 30 (0.000501)\n",
      "166. feature 147 (0.000498)\n",
      "167. feature 202 (0.000496)\n",
      "168. feature 256 (0.000493)\n",
      "169. feature 96 (0.000490)\n",
      "170. feature 302 (0.000488)\n",
      "171. feature 323 (0.000486)\n",
      "172. feature 310 (0.000484)\n",
      "173. feature 88 (0.000481)\n",
      "174. feature 99 (0.000469)\n",
      "175. feature 287 (0.000468)\n",
      "176. feature 306 (0.000452)\n",
      "177. feature 343 (0.000446)\n",
      "178. feature 260 (0.000441)\n",
      "179. feature 284 (0.000440)\n",
      "180. feature 261 (0.000429)\n",
      "181. feature 98 (0.000420)\n",
      "182. feature 102 (0.000419)\n",
      "183. feature 345 (0.000412)\n",
      "184. feature 293 (0.000405)\n",
      "185. feature 285 (0.000403)\n",
      "186. feature 100 (0.000397)\n",
      "187. feature 267 (0.000393)\n",
      "188. feature 160 (0.000391)\n",
      "189. feature 197 (0.000390)\n",
      "190. feature 201 (0.000389)\n",
      "191. feature 259 (0.000389)\n",
      "192. feature 36 (0.000382)\n",
      "193. feature 214 (0.000380)\n",
      "194. feature 303 (0.000378)\n",
      "195. feature 305 (0.000376)\n",
      "196. feature 178 (0.000373)\n",
      "197. feature 249 (0.000350)\n",
      "198. feature 77 (0.000344)\n",
      "199. feature 95 (0.000342)\n",
      "200. feature 320 (0.000342)\n",
      "201. feature 199 (0.000334)\n",
      "202. feature 304 (0.000329)\n",
      "203. feature 292 (0.000328)\n",
      "204. feature 294 (0.000325)\n",
      "205. feature 198 (0.000324)\n",
      "206. feature 295 (0.000323)\n",
      "207. feature 207 (0.000319)\n",
      "208. feature 179 (0.000316)\n",
      "209. feature 232 (0.000308)\n",
      "210. feature 296 (0.000307)\n",
      "211. feature 71 (0.000307)\n",
      "212. feature 309 (0.000305)\n",
      "213. feature 301 (0.000302)\n",
      "214. feature 308 (0.000300)\n",
      "215. feature 164 (0.000296)\n",
      "216. feature 353 (0.000294)\n",
      "217. feature 216 (0.000289)\n",
      "218. feature 307 (0.000280)\n",
      "219. feature 215 (0.000279)\n",
      "220. feature 300 (0.000273)\n",
      "221. feature 282 (0.000265)\n",
      "222. feature 233 (0.000264)\n",
      "223. feature 288 (0.000259)\n",
      "224. feature 268 (0.000256)\n",
      "225. feature 276 (0.000252)\n",
      "226. feature 86 (0.000248)\n",
      "227. feature 290 (0.000248)\n",
      "228. feature 123 (0.000244)\n",
      "229. feature 286 (0.000237)\n",
      "230. feature 150 (0.000231)\n",
      "231. feature 128 (0.000226)\n",
      "232. feature 45 (0.000218)\n",
      "233. feature 289 (0.000214)\n",
      "234. feature 90 (0.000213)\n",
      "235. feature 161 (0.000210)\n",
      "236. feature 281 (0.000204)\n",
      "237. feature 263 (0.000190)\n",
      "238. feature 114 (0.000187)\n",
      "239. feature 354 (0.000183)\n",
      "240. feature 62 (0.000176)\n",
      "241. feature 217 (0.000175)\n",
      "242. feature 20 (0.000173)\n",
      "243. feature 277 (0.000164)\n",
      "244. feature 278 (0.000157)\n",
      "245. feature 240 (0.000154)\n",
      "246. feature 81 (0.000143)\n",
      "247. feature 125 (0.000139)\n",
      "248. feature 148 (0.000131)\n",
      "249. feature 115 (0.000131)\n",
      "250. feature 262 (0.000127)\n",
      "251. feature 203 (0.000122)\n",
      "252. feature 143 (0.000121)\n",
      "253. feature 166 (0.000120)\n",
      "254. feature 279 (0.000113)\n",
      "255. feature 280 (0.000110)\n",
      "256. feature 142 (0.000099)\n",
      "257. feature 239 (0.000098)\n",
      "258. feature 57 (0.000096)\n",
      "259. feature 84 (0.000092)\n",
      "260. feature 92 (0.000089)\n",
      "261. feature 78 (0.000085)\n",
      "262. feature 129 (0.000083)\n",
      "263. feature 126 (0.000076)\n",
      "264. feature 127 (0.000076)\n",
      "265. feature 131 (0.000072)\n",
      "266. feature 116 (0.000069)\n",
      "267. feature 157 (0.000068)\n",
      "268. feature 146 (0.000068)\n",
      "269. feature 2 (0.000061)\n",
      "270. feature 248 (0.000060)\n",
      "271. feature 103 (0.000059)\n",
      "272. feature 141 (0.000058)\n",
      "273. feature 94 (0.000055)\n",
      "274. feature 80 (0.000055)\n",
      "275. feature 339 (0.000047)\n",
      "276. feature 213 (0.000042)\n",
      "277. feature 132 (0.000040)\n",
      "278. feature 149 (0.000038)\n",
      "279. feature 104 (0.000035)\n",
      "280. feature 67 (0.000031)\n",
      "281. feature 76 (0.000026)\n",
      "282. feature 236 (0.000023)\n",
      "283. feature 151 (0.000022)\n",
      "284. feature 360 (0.000020)\n",
      "285. feature 365 (0.000017)\n",
      "286. feature 247 (0.000016)\n",
      "287. feature 105 (0.000015)\n",
      "288. feature 364 (0.000015)\n",
      "289. feature 363 (0.000014)\n",
      "290. feature 334 (0.000013)\n",
      "291. feature 133 (0.000012)\n",
      "292. feature 361 (0.000012)\n",
      "293. feature 362 (0.000011)\n",
      "294. feature 136 (0.000011)\n",
      "295. feature 251 (0.000009)\n",
      "296. feature 75 (0.000008)\n",
      "297. feature 173 (0.000007)\n",
      "298. feature 337 (0.000007)\n",
      "299. feature 274 (0.000006)\n",
      "300. feature 210 (0.000005)\n",
      "301. feature 165 (0.000005)\n",
      "302. feature 111 (0.000004)\n",
      "303. feature 135 (0.000004)\n",
      "304. feature 65 (0.000004)\n",
      "305. feature 298 (0.000003)\n",
      "306. feature 188 (0.000003)\n",
      "307. feature 340 (0.000003)\n",
      "308. feature 186 (0.000002)\n",
      "309. feature 117 (0.000002)\n",
      "310. feature 191 (0.000002)\n",
      "311. feature 193 (0.000002)\n",
      "312. feature 63 (0.000001)\n",
      "313. feature 252 (0.000001)\n",
      "314. feature 107 (0.000001)\n",
      "315. feature 174 (0.000001)\n",
      "316. feature 335 (0.000001)\n",
      "317. feature 297 (0.000000)\n",
      "318. feature 209 (0.000000)\n",
      "319. feature 145 (0.000000)\n",
      "320. feature 299 (0.000000)\n",
      "321. feature 181 (0.000000)\n",
      "322. feature 112 (0.000000)\n",
      "323. feature 235 (0.000000)\n",
      "324. feature 238 (0.000000)\n",
      "325. feature 242 (0.000000)\n",
      "326. feature 243 (0.000000)\n",
      "327. feature 245 (0.000000)\n",
      "328. feature 119 (0.000000)\n",
      "329. feature 250 (0.000000)\n",
      "330. feature 118 (0.000000)\n",
      "331. feature 253 (0.000000)\n",
      "332. feature 10 (0.000000)\n",
      "333. feature 21 (0.000000)\n",
      "334. feature 72 (0.000000)\n",
      "335. feature 254 (0.000000)\n",
      "336. feature 272 (0.000000)\n",
      "337. feature 273 (0.000000)\n",
      "338. feature 110 (0.000000)\n",
      "339. feature 109 (0.000000)\n",
      "340. feature 108 (0.000000)\n",
      "341. feature 275 (0.000000)\n",
      "342. feature 234 (0.000000)\n",
      "343. feature 211 (0.000000)\n",
      "344. feature 22 (0.000000)\n",
      "345. feature 212 (0.000000)\n",
      "346. feature 52 (0.000000)\n",
      "347. feature 184 (0.000000)\n",
      "348. feature 144 (0.000000)\n",
      "349. feature 41 (0.000000)\n",
      "350. feature 106 (0.000000)\n",
      "351. feature 139 (0.000000)\n",
      "352. feature 137 (0.000000)\n",
      "353. feature 134 (0.000000)\n",
      "354. feature 189 (0.000000)\n",
      "355. feature 124 (0.000000)\n",
      "356. feature 122 (0.000000)\n",
      "357. feature 341 (0.000000)\n",
      "358. feature 342 (0.000000)\n",
      "359. feature 121 (0.000000)\n",
      "360. feature 195 (0.000000)\n",
      "361. feature 196 (0.000000)\n",
      "362. feature 206 (0.000000)\n",
      "363. feature 120 (0.000000)\n",
      "364. feature 167 (0.000000)\n",
      "365. feature 140 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "#ACTUAL MODEL CODE\n",
    "\n",
    "X = data[:, 3:382]\n",
    "y = data[:, 382]\n",
    "\n",
    "X_test = test_data[:, 3:382]\n",
    "\n",
    "X[X < 0] = -1\n",
    "X = np.delete(X, real_bad_indices_2, axis = 1)\n",
    "imp = Imputer(missing_values=-1, strategy='mean')\n",
    "X = imp.fit_transform(X)\n",
    "\n",
    "#X = np.delete(X, i_love_indices[len(i_love_indices) - 250:], axis=1)\n",
    "\n",
    "\n",
    "#X_test[X_test < 0] = -1\n",
    "X_test = np.delete(X_test, real_bad_indices_2,axis = 1)\n",
    "imp = Imputer(missing_values=-1, strategy='mean')\n",
    "X_test = imp.fit_transform(X_test)\n",
    "\n",
    "#X_test = np.delete(X_test, i_love_indices[len(i_love_indices) - 250:], axis=1)\n",
    "\n",
    "#print(X.shape)\n",
    "clf = RandomForestClassifier(n_estimators = 190, min_samples_leaf = 20)\n",
    "clf.fit(X, y)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "#Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f, indices[f], importances[indices[f]]))\n",
    "    #print(X_train[:, indices[f]])\n",
    "    \n",
    "i_love_indices = indices\n",
    "\n",
    "results = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "results[results < 0.03] = 0\n",
    "results[results > 0.67] = 1\n",
    "with open('output_1.csv', 'w') as output:\n",
    "    output.write('id,target\\n')\n",
    "    for i in range(len(test_data)):\n",
    "        output.write(str(int(test_data[i][0])) + ',' + str(results[i]) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8198093  0.80898397 0.79962917 0.79021625 0.78625062 0.78174169\n",
      " 0.77853367 0.77287553 0.77251965 0.76782463 0.76634991 0.76543805\n",
      " 0.76533352 0.76396577 0.76154461 0.75544078 0.75501377 0.75472866\n",
      " 0.7546399  0.75458857 0.75358048 0.75243728 0.75224463 0.7498976\n",
      " 0.74829219 0.74430351 0.74257807 0.74173416 0.74158917 0.74113962\n",
      " 0.73908588 0.73775042 0.73564811 0.73533103 0.73526861 0.73502455\n",
      " 0.73359814 0.73351175 0.73261683 0.73128456 0.72906647 0.72233281\n",
      " 0.72222513 0.7200193  0.7192253  0.71918656 0.71897341 0.71838145\n",
      " 0.71754991 0.71695955 0.71683241 0.71653549 0.71561504 0.71463297\n",
      " 0.7145744  0.71417955 0.71383217 0.71223372 0.71113641 0.71004102\n",
      " 0.70977054 0.70806521 0.7076079  0.70668292 0.70667916 0.70656725\n",
      " 0.70564052 0.70539752 0.70490761 0.70478262 0.70475867 0.70447995\n",
      " 0.70446921 0.70346516 0.70319091 0.70317852 0.70299224 0.70291811\n",
      " 0.70213975 0.70203857 0.70186658 0.70141378 0.70111054 0.7006842\n",
      " 0.70051245 0.70028475 0.69987121 0.69960211 0.69884543 0.69878389\n",
      " 0.69841817 0.69743702 0.69682552 0.69671847 0.69669214 0.69650423\n",
      " 0.69620605 0.69605726 0.69468834 0.69415874 0.69375805 0.69342542\n",
      " 0.69332111 0.692934   0.69288878 0.69256172 0.6923803  0.69212319\n",
      " 0.69183763 0.69156186 0.69147395 0.69111863 0.69108108 0.69106604\n",
      " 0.69052419 0.69043799 0.69036867 0.6903036  0.69025606 0.6902116\n",
      " 0.69019645 0.68955113 0.68919276 0.68913481 0.68903362 0.68873365\n",
      " 0.68856146 0.6878742  0.68771148 0.68761283 0.68708706 0.68706852\n",
      " 0.68701615 0.68671901 0.68604571 0.68539655 0.68446284 0.68441443\n",
      " 0.68378376 0.6835921  0.68336966 0.6832783  0.68311927 0.68306499\n",
      " 0.68267288 0.68173496 0.681688   0.6815027  0.68137141 0.68123768\n",
      " 0.6811592  0.68043336 0.68029614 0.67949848 0.67941518 0.67929582\n",
      " 0.67923569 0.67908377 0.67890448 0.67882728 0.67841723 0.67829007\n",
      " 0.67817308 0.67807106 0.67800632 0.67796617 0.67779293 0.6775005\n",
      " 0.67730213 0.67715647 0.67704394 0.6766525  0.67664756 0.67644237\n",
      " 0.67557175 0.67521053 0.67478057 0.67472673 0.67467059 0.67420578]\n"
     ]
    }
   ],
   "source": [
    "test = np.copy(results[:, 1])\n",
    "\n",
    "test.sort()\n",
    "print(test[::-1][:180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 42  35   8 333   3 194 187 328 192  33 317 325  37   9  51 225 229 228\n",
      "  24 331  25 327  26 332 226 244 322 255 185 319 321 230  31  46 231 318\n",
      "  12 221  43  60 315 336 190  11  27 316  13  58  48  32   4 358  55  53\n",
      " 224  14  28  49  61  93 330  70  54  23  44  91 355 152  59 171  17  50\n",
      "  15  56  39 314 227 180 182  79   0 348  82 223 349 172 352 359 183  69\n",
      " 324 156  38  66 163 313  29  73 356 241 175  68  74 329   5 170 208  47\n",
      " 159 205 218 347  18  16 169 283 176 168  87 338 177 271 357 158   7  40\n",
      " 220 246  97  89 204 155 138 222 101  19  64 265 346 270  34 200 219 344\n",
      " 326 113   1  88 264  83 351 102 343 266 237 312  96   6 306 147 311 258\n",
      " 153 295  99 320 162 261 154 285 256 302 202 310 164  30  85 267 130 263\n",
      "  95 291 350 293 345  71  36 269 197  98 303 100 260 232 179 305 323 294\n",
      "  77 214 259 308 160 296 249 215 284 304 309 128 198 207 199  86 257 150\n",
      " 268 287 286 307 300 353 201 354 216 301 233 178  90 289  45 288 290 114\n",
      " 282 123 277 292 217  62 161  20   2 240 129 262 278 115 148 146 125 279\n",
      " 281  84  81 141 127 126 166 143 203  78  80 280 276 157  94  92 131 142\n",
      " 103 213  67  57 116 239 339 149  75 132 248 236 363 247  65 104 361 251\n",
      " 117 297 133 250 136  63 134  76 173 365 362 135 252 151 334 299 165 337\n",
      " 275 340 186 298 364 145 110 209 188 191 243 242 245  21  22 238 112 111\n",
      " 253 109 108 107 234  10 360 106 254 272 273 105 274 235 139 118 335 137\n",
      " 144  72 167 124 122 174 181  52 184 121 189 212 120  41 119 341 193 342\n",
      " 195 140 206 210 211 196]\n"
     ]
    }
   ],
   "source": [
    "print(i_love_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
