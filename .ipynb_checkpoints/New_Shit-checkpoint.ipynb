{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def eval_tree_based_model_max_depth(clf, max_depth, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function evaluates the given classifier (either a decision tree or random forest) at all of the \n",
    "    maximum tree depth parameters in the vector max_depth, using the given training and testing\n",
    "    data. It returns two vector, with the training and testing classification errors.\n",
    "    \n",
    "    Inputs:\n",
    "        clf: either a decision tree or random forest classifier object\n",
    "        max_depth: a (T, ) vector of all the max_depth stopping condition parameters \n",
    "                            to test, where T is the number of parameters to test\n",
    "        X_train: (N, D) matrix of training samples.\n",
    "        y_train: (N, ) vector of training labels.\n",
    "        X_test: (N, D) matrix of test samples\n",
    "        y_test: (N, ) vector of test labels\n",
    "    Output:\n",
    "        train_err: (T, ) vector of classification errors on the training data\n",
    "        test_err: (T, ) vector of classification errors on the test data\n",
    "    \"\"\"\n",
    "    training_errors = []\n",
    "    test_errors = []\n",
    "    # evaluates the tree classifier for the desired number of max_tree_depth values values \n",
    "    for i in range(len(max_depth)):\n",
    "        print(max_depth[i])\n",
    "        clf.set_params(min_samples_leaf = max_depth[i])\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_train_predict = clf.predict_proba(X_train)[:, 1]\n",
    "        y_test_predict = clf.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        training_errors.append(roc_auc_score(y_train, y_train_predict))\n",
    "        test_errors.append(roc_auc_score( y_test, y_test_predict))\n",
    "    return np.array(training_errors), np.array(test_errors)\n",
    "\n",
    "def classification_err(y, real_y):\n",
    "    \"\"\"\n",
    "    This function returns the classification error between two equally-sized vectors of \n",
    "    labels; this is the fraction of samples for which the labels differ.\n",
    "    \n",
    "    Inputs:\n",
    "        y: (N, ) shaped array of predicted labels\n",
    "        real_y: (N, ) shaped array of true labels\n",
    "    Output:\n",
    "        Scalar classification error\n",
    "    \"\"\"\n",
    "    return sum(y != real_y)/len(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(open(\"train_2008.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "test_data = np.loadtxt(open(\"test_2008.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "X = data[:, 3:382]\n",
    "y = data[:, 382]\n",
    "print(y)\n",
    "X_test = test_data[:, 3:382]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64663"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X[:, 10]<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64667, 379)\n",
      "(16000, 379)\n",
      "[9, 11, 126, 127, 128, 132, 133, 134]\n",
      "[9, 10, 11, 124, 125, 126, 127, 128, 131, 132, 133, 134, 215]\n",
      "(64667, 371)\n",
      "(16000, 371)\n",
      "(64667, 371)\n",
      "(16000, 366)\n"
     ]
    }
   ],
   "source": [
    "X[X < 0] = -1\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "bad_indices = []\n",
    "real_bad_indices = []\n",
    "for i in range(np.shape(X)[1]):\n",
    "    num_less = X[:,i] < 0\n",
    "    #bad_indices.append(float(sum(num_less))/float(len(num_less)))\n",
    "    if sum(num_less)/len(num_less) == 1:\n",
    "        real_bad_indices.append(i)\n",
    "        \n",
    "print(real_bad_indices)\n",
    "# print(bad_indices[45])\n",
    "\n",
    "real_bad_indices_2 = []\n",
    "for i in range(np.shape(X_test)[1]):\n",
    "    num_less = X_test[:,i] < 0\n",
    "    if sum(num_less)/len(num_less) == 1:\n",
    "        real_bad_indices_2.append(i)\n",
    "            \n",
    "print(real_bad_indices_2)  \n",
    "X = np.delete(X, real_bad_indices, axis = 1)\n",
    "X_test = np.delete(X_test, real_bad_indices,axis = 1)\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "imp = Imputer(missing_values=-1, strategy='mean')\n",
    "X = imp.fit_transform(X)\n",
    "imp = Imputer(missing_values=-1, strategy='mean')\n",
    "X_test = imp.fit_transform(X_test)\n",
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 11, 124, 125, 126, 127, 128, 131, 132, 133, 134, 215]\n",
      "(64667, 379)\n",
      "[-1. -1. -1. ... -1. -1. -1.]\n",
      "[16.  6. 15. ... 16. 12. -2.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "0. feature 43 (0.090642)\n",
      "1. feature 36 (0.045307)\n",
      "2. feature 8 (0.037514)\n",
      "3. feature 3 (0.032346)\n",
      "4. feature 338 (0.030078)\n",
      "5. feature 196 (0.023566)\n",
      "6. feature 333 (0.021514)\n",
      "7. feature 191 (0.020531)\n",
      "8. feature 34 (0.017064)\n",
      "9. feature 198 (0.017061)\n",
      "10. feature 322 (0.015240)\n",
      "11. feature 330 (0.014325)\n",
      "12. feature 52 (0.014005)\n",
      "13. feature 337 (0.013403)\n",
      "14. feature 230 (0.013313)\n",
      "15. feature 10 (0.013311)\n",
      "16. feature 234 (0.013176)\n",
      "17. feature 336 (0.013073)\n",
      "18. feature 25 (0.012993)\n",
      "19. feature 233 (0.012968)\n",
      "20. feature 26 (0.012940)\n",
      "21. feature 38 (0.012894)\n",
      "22. feature 332 (0.012601)\n",
      "23. feature 231 (0.011754)\n",
      "24. feature 27 (0.011636)\n",
      "25. feature 324 (0.010717)\n",
      "26. feature 189 (0.010245)\n",
      "27. feature 260 (0.010125)\n",
      "28. feature 47 (0.010111)\n",
      "29. feature 249 (0.009454)\n",
      "30. feature 327 (0.009317)\n",
      "31. feature 321 (0.009246)\n",
      "32. feature 61 (0.009098)\n",
      "33. feature 235 (0.008754)\n",
      "34. feature 320 (0.008717)\n",
      "35. feature 236 (0.008625)\n",
      "36. feature 32 (0.008552)\n",
      "37. feature 326 (0.008056)\n",
      "38. feature 44 (0.007609)\n",
      "39. feature 13 (0.007361)\n",
      "40. feature 28 (0.007072)\n",
      "41. feature 319 (0.006946)\n",
      "42. feature 341 (0.006908)\n",
      "43. feature 226 (0.006813)\n",
      "44. feature 194 (0.006612)\n",
      "45. feature 323 (0.006611)\n",
      "46. feature 12 (0.006463)\n",
      "47. feature 14 (0.006353)\n",
      "48. feature 335 (0.006186)\n",
      "49. feature 4 (0.006164)\n",
      "50. feature 59 (0.006086)\n",
      "51. feature 49 (0.005961)\n",
      "52. feature 363 (0.005885)\n",
      "53. feature 56 (0.005252)\n",
      "54. feature 33 (0.005068)\n",
      "55. feature 55 (0.004947)\n",
      "56. feature 29 (0.004810)\n",
      "57. feature 50 (0.004739)\n",
      "58. feature 15 (0.004690)\n",
      "59. feature 62 (0.004644)\n",
      "60. feature 45 (0.004611)\n",
      "61. feature 94 (0.004503)\n",
      "62. feature 24 (0.004473)\n",
      "63. feature 229 (0.004443)\n",
      "64. feature 54 (0.004401)\n",
      "65. feature 71 (0.004398)\n",
      "66. feature 186 (0.004296)\n",
      "67. feature 354 (0.004094)\n",
      "68. feature 92 (0.004075)\n",
      "69. feature 16 (0.003770)\n",
      "70. feature 156 (0.003635)\n",
      "71. feature 18 (0.003520)\n",
      "72. feature 175 (0.003459)\n",
      "73. feature 40 (0.003458)\n",
      "74. feature 232 (0.003382)\n",
      "75. feature 60 (0.003315)\n",
      "76. feature 360 (0.003300)\n",
      "77. feature 83 (0.003226)\n",
      "78. feature 0 (0.003197)\n",
      "79. feature 57 (0.003114)\n",
      "80. feature 51 (0.003103)\n",
      "81. feature 80 (0.003071)\n",
      "82. feature 353 (0.003045)\n",
      "83. feature 228 (0.002908)\n",
      "84. feature 187 (0.002907)\n",
      "85. feature 176 (0.002815)\n",
      "86. feature 184 (0.002562)\n",
      "87. feature 357 (0.002559)\n",
      "88. feature 364 (0.002391)\n",
      "89. feature 160 (0.002289)\n",
      "90. feature 39 (0.002222)\n",
      "91. feature 30 (0.002090)\n",
      "92. feature 69 (0.001979)\n",
      "93. feature 70 (0.001956)\n",
      "94. feature 167 (0.001929)\n",
      "95. feature 334 (0.001757)\n",
      "96. feature 48 (0.001725)\n",
      "97. feature 179 (0.001713)\n",
      "98. feature 329 (0.001695)\n",
      "99. feature 213 (0.001638)\n",
      "100. feature 361 (0.001603)\n",
      "101. feature 276 (0.001589)\n",
      "102. feature 352 (0.001501)\n",
      "103. feature 67 (0.001454)\n",
      "104. feature 163 (0.001419)\n",
      "105. feature 288 (0.001405)\n",
      "106. feature 5 (0.001396)\n",
      "107. feature 17 (0.001367)\n",
      "108. feature 65 (0.001333)\n",
      "109. feature 74 (0.001322)\n",
      "110. feature 343 (0.001256)\n",
      "111. feature 181 (0.001213)\n",
      "112. feature 318 (0.001207)\n",
      "113. feature 210 (0.001198)\n",
      "114. feature 246 (0.001176)\n",
      "115. feature 88 (0.001148)\n",
      "116. feature 19 (0.001147)\n",
      "117. feature 172 (0.001119)\n",
      "118. feature 173 (0.001108)\n",
      "119. feature 75 (0.001056)\n",
      "120. feature 174 (0.001056)\n",
      "121. feature 223 (0.001051)\n",
      "122. feature 180 (0.001026)\n",
      "123. feature 162 (0.001017)\n",
      "124. feature 356 (0.000989)\n",
      "125. feature 7 (0.000967)\n",
      "126. feature 270 (0.000963)\n",
      "127. feature 251 (0.000960)\n",
      "128. feature 41 (0.000943)\n",
      "129. feature 351 (0.000939)\n",
      "130. feature 90 (0.000894)\n",
      "131. feature 98 (0.000886)\n",
      "132. feature 102 (0.000857)\n",
      "133. feature 225 (0.000825)\n",
      "134. feature 35 (0.000822)\n",
      "135. feature 224 (0.000799)\n",
      "136. feature 159 (0.000793)\n",
      "137. feature 269 (0.000760)\n",
      "138. feature 20 (0.000760)\n",
      "139. feature 362 (0.000753)\n",
      "140. feature 114 (0.000747)\n",
      "141. feature 331 (0.000744)\n",
      "142. feature 158 (0.000738)\n",
      "143. feature 134 (0.000734)\n",
      "144. feature 261 (0.000727)\n",
      "145. feature 97 (0.000708)\n",
      "146. feature 204 (0.000707)\n",
      "147. feature 272 (0.000702)\n",
      "148. feature 142 (0.000674)\n",
      "149. feature 209 (0.000630)\n",
      "150. feature 86 (0.000620)\n",
      "151. feature 349 (0.000614)\n",
      "152. feature 157 (0.000611)\n",
      "153. feature 151 (0.000610)\n",
      "154. feature 1 (0.000602)\n",
      "155. feature 242 (0.000578)\n",
      "156. feature 328 (0.000577)\n",
      "157. feature 227 (0.000577)\n",
      "158. feature 275 (0.000573)\n",
      "159. feature 296 (0.000571)\n",
      "160. feature 316 (0.000562)\n",
      "161. feature 355 (0.000544)\n",
      "162. feature 166 (0.000530)\n",
      "163. feature 271 (0.000525)\n",
      "164. feature 315 (0.000510)\n",
      "165. feature 89 (0.000508)\n",
      "166. feature 100 (0.000506)\n",
      "167. feature 298 (0.000500)\n",
      "168. feature 84 (0.000498)\n",
      "169. feature 350 (0.000494)\n",
      "170. feature 274 (0.000488)\n",
      "171. feature 206 (0.000482)\n",
      "172. feature 307 (0.000479)\n",
      "173. feature 103 (0.000474)\n",
      "174. feature 263 (0.000465)\n",
      "175. feature 262 (0.000458)\n",
      "176. feature 348 (0.000455)\n",
      "177. feature 311 (0.000452)\n",
      "178. feature 99 (0.000444)\n",
      "179. feature 6 (0.000444)\n",
      "180. feature 308 (0.000436)\n",
      "181. feature 265 (0.000430)\n",
      "182. feature 317 (0.000427)\n",
      "183. feature 31 (0.000426)\n",
      "184. feature 203 (0.000426)\n",
      "185. feature 268 (0.000424)\n",
      "186. feature 183 (0.000416)\n",
      "187. feature 266 (0.000414)\n",
      "188. feature 101 (0.000412)\n",
      "189. feature 168 (0.000396)\n",
      "190. feature 325 (0.000393)\n",
      "191. feature 72 (0.000385)\n",
      "192. feature 37 (0.000385)\n",
      "193. feature 78 (0.000378)\n",
      "194. feature 313 (0.000376)\n",
      "195. feature 273 (0.000374)\n",
      "196. feature 201 (0.000372)\n",
      "197. feature 300 (0.000370)\n",
      "198. feature 154 (0.000360)\n",
      "199. feature 219 (0.000357)\n",
      "200. feature 237 (0.000357)\n",
      "201. feature 264 (0.000354)\n",
      "202. feature 202 (0.000352)\n",
      "203. feature 290 (0.000352)\n",
      "204. feature 254 (0.000348)\n",
      "205. feature 96 (0.000341)\n",
      "206. feature 221 (0.000335)\n",
      "207. feature 310 (0.000324)\n",
      "208. feature 309 (0.000318)\n",
      "209. feature 306 (0.000317)\n",
      "210. feature 314 (0.000314)\n",
      "211. feature 305 (0.000310)\n",
      "212. feature 164 (0.000309)\n",
      "213. feature 289 (0.000302)\n",
      "214. feature 220 (0.000292)\n",
      "215. feature 312 (0.000289)\n",
      "216. feature 205 (0.000287)\n",
      "217. feature 132 (0.000280)\n",
      "218. feature 182 (0.000279)\n",
      "219. feature 299 (0.000276)\n",
      "220. feature 87 (0.000276)\n",
      "221. feature 358 (0.000271)\n",
      "222. feature 292 (0.000259)\n",
      "223. feature 212 (0.000254)\n",
      "224. feature 238 (0.000252)\n",
      "225. feature 301 (0.000252)\n",
      "226. feature 291 (0.000242)\n",
      "227. feature 293 (0.000242)\n",
      "228. feature 91 (0.000239)\n",
      "229. feature 282 (0.000228)\n",
      "230. feature 46 (0.000227)\n",
      "231. feature 295 (0.000203)\n",
      "232. feature 165 (0.000201)\n",
      "233. feature 359 (0.000187)\n",
      "234. feature 294 (0.000182)\n",
      "235. feature 115 (0.000182)\n",
      "236. feature 283 (0.000180)\n",
      "237. feature 297 (0.000178)\n",
      "238. feature 286 (0.000167)\n",
      "239. feature 222 (0.000164)\n",
      "240. feature 245 (0.000161)\n",
      "241. feature 127 (0.000158)\n",
      "242. feature 281 (0.000158)\n",
      "243. feature 63 (0.000155)\n",
      "244. feature 152 (0.000148)\n",
      "245. feature 129 (0.000141)\n",
      "246. feature 267 (0.000137)\n",
      "247. feature 133 (0.000135)\n",
      "248. feature 147 (0.000131)\n",
      "249. feature 208 (0.000128)\n",
      "250. feature 82 (0.000119)\n",
      "251. feature 93 (0.000118)\n",
      "252. feature 284 (0.000117)\n",
      "253. feature 285 (0.000115)\n",
      "254. feature 21 (0.000114)\n",
      "255. feature 116 (0.000108)\n",
      "256. feature 131 (0.000106)\n",
      "257. feature 146 (0.000105)\n",
      "258. feature 150 (0.000104)\n",
      "259. feature 2 (0.000090)\n",
      "260. feature 81 (0.000088)\n",
      "261. feature 287 (0.000087)\n",
      "262. feature 170 (0.000084)\n",
      "263. feature 79 (0.000075)\n",
      "264. feature 145 (0.000070)\n",
      "265. feature 218 (0.000069)\n",
      "266. feature 95 (0.000059)\n",
      "267. feature 58 (0.000058)\n",
      "268. feature 153 (0.000057)\n",
      "269. feature 85 (0.000057)\n",
      "270. feature 136 (0.000052)\n",
      "271. feature 130 (0.000052)\n",
      "272. feature 253 (0.000046)\n",
      "273. feature 68 (0.000044)\n",
      "274. feature 344 (0.000040)\n",
      "275. feature 161 (0.000040)\n",
      "276. feature 77 (0.000035)\n",
      "277. feature 244 (0.000028)\n",
      "278. feature 135 (0.000027)\n",
      "279. feature 367 (0.000019)\n",
      "280. feature 252 (0.000017)\n",
      "281. feature 185 (0.000017)\n",
      "282. feature 117 (0.000017)\n",
      "283. feature 366 (0.000016)\n",
      "284. feature 155 (0.000014)\n",
      "285. feature 257 (0.000013)\n",
      "286. feature 342 (0.000012)\n",
      "287. feature 140 (0.000010)\n",
      "288. feature 105 (0.000010)\n",
      "289. feature 256 (0.000010)\n",
      "290. feature 76 (0.000010)\n",
      "291. feature 369 (0.000009)\n",
      "292. feature 104 (0.000009)\n",
      "293. feature 241 (0.000008)\n",
      "294. feature 66 (0.000008)\n",
      "295. feature 304 (0.000008)\n",
      "296. feature 365 (0.000008)\n",
      "297. feature 192 (0.000007)\n",
      "298. feature 368 (0.000007)\n",
      "299. feature 303 (0.000006)\n",
      "300. feature 339 (0.000005)\n",
      "301. feature 188 (0.000005)\n",
      "302. feature 190 (0.000004)\n",
      "303. feature 370 (0.000004)\n",
      "304. feature 138 (0.000004)\n",
      "305. feature 177 (0.000004)\n",
      "306. feature 149 (0.000003)\n",
      "307. feature 109 (0.000003)\n",
      "308. feature 110 (0.000002)\n",
      "309. feature 195 (0.000002)\n",
      "310. feature 302 (0.000002)\n",
      "311. feature 345 (0.000002)\n",
      "312. feature 340 (0.000002)\n",
      "313. feature 106 (0.000002)\n",
      "314. feature 215 (0.000002)\n",
      "315. feature 197 (0.000002)\n",
      "316. feature 169 (0.000001)\n",
      "317. feature 214 (0.000001)\n",
      "318. feature 23 (0.000000)\n",
      "319. feature 277 (0.000000)\n",
      "320. feature 118 (0.000000)\n",
      "321. feature 255 (0.000000)\n",
      "322. feature 22 (0.000000)\n",
      "323. feature 113 (0.000000)\n",
      "324. feature 250 (0.000000)\n",
      "325. feature 258 (0.000000)\n",
      "326. feature 259 (0.000000)\n",
      "327. feature 126 (0.000000)\n",
      "328. feature 112 (0.000000)\n",
      "329. feature 278 (0.000000)\n",
      "330. feature 279 (0.000000)\n",
      "331. feature 280 (0.000000)\n",
      "332. feature 247 (0.000000)\n",
      "333. feature 111 (0.000000)\n",
      "334. feature 11 (0.000000)\n",
      "335. feature 9 (0.000000)\n",
      "336. feature 346 (0.000000)\n",
      "337. feature 347 (0.000000)\n",
      "338. feature 108 (0.000000)\n",
      "339. feature 107 (0.000000)\n",
      "340. feature 248 (0.000000)\n",
      "341. feature 217 (0.000000)\n",
      "342. feature 243 (0.000000)\n",
      "343. feature 123 (0.000000)\n",
      "344. feature 64 (0.000000)\n",
      "345. feature 137 (0.000000)\n",
      "346. feature 139 (0.000000)\n",
      "347. feature 125 (0.000000)\n",
      "348. feature 141 (0.000000)\n",
      "349. feature 143 (0.000000)\n",
      "350. feature 144 (0.000000)\n",
      "351. feature 148 (0.000000)\n",
      "352. feature 124 (0.000000)\n",
      "353. feature 53 (0.000000)\n",
      "354. feature 171 (0.000000)\n",
      "355. feature 178 (0.000000)\n",
      "356. feature 122 (0.000000)\n",
      "357. feature 119 (0.000000)\n",
      "358. feature 121 (0.000000)\n",
      "359. feature 73 (0.000000)\n",
      "360. feature 120 (0.000000)\n",
      "361. feature 42 (0.000000)\n",
      "362. feature 199 (0.000000)\n",
      "363. feature 200 (0.000000)\n",
      "364. feature 207 (0.000000)\n",
      "365. feature 211 (0.000000)\n",
      "366. feature 216 (0.000000)\n",
      "367. feature 128 (0.000000)\n",
      "368. feature 239 (0.000000)\n",
      "369. feature 240 (0.000000)\n",
      "370. feature 193 (0.000000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 371 and input n_features is 366 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a2c998816e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#     print('Test error minimized at max_depth =', np.argmax(test_err), max_depth[np.argmax(test_err)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    382\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 371 and input n_features is 366 "
     ]
    }
   ],
   "source": [
    "# kf = KFold(n_splits = 5, shuffle = True) \n",
    "\n",
    "# all_predictions = np.zeros(5)\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_val = X[train_index], X[test_index]\n",
    "#     y_train, y_val = y[train_index], y[test_index]\n",
    "    \n",
    "#     #fprint(np.shape(y_test))\n",
    "#     scaler = StandardScaler()\n",
    "#     # Fit on training set only.\n",
    "#     scaler.fit(X_train)\n",
    "#     # Apply transform to both the training set and the test set.\n",
    "#     X_train = scaler.transform(X_train)\n",
    "#     X_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X)\n",
    "# X = scaler.transform(X)\n",
    "# X_test = scaler.transform(X_test)\n",
    "    \n",
    "# pca = PCA(n_components = 250)\n",
    "# pca.fit(X)\n",
    "# X = pca.transform(X)\n",
    "# X_test = pca.transform(X_test)\n",
    "#      X_val = pca.transform(X_val)\n",
    "    \n",
    "clf = RandomForestClassifier(n_estimators = 190, min_samples_leaf = 20)\n",
    "#   clf = svm.SVC(probability = True)\n",
    "#    clf.fit(X_train, y_train)\n",
    "#     print(clf.classes_)\n",
    "clf.fit(X, y)\n",
    "    \n",
    "#     predictions = clf.predict_proba(X_val)[:, 1]\n",
    "#     print(sum(np.floor(2.0*predictions) != np.array(y_val))/len(predictions))\n",
    "#     print(roc_auc_score(y_val, predictions))\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "#Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f, indices[f], importances[indices[f]]))\n",
    "    #print(X_train[:, indices[f]])\n",
    "\n",
    "\n",
    " \n",
    "#     max_depth = np.arange(10, 200, 20)\n",
    "#     train_err, test_err = eval_tree_based_model_max_depth(clf, max_depth, X_train, \n",
    "#                                                             y_train, X_val, y_val)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(max_depth, test_err, label='Testing error')\n",
    "#     plt.plot(max_depth, train_err, label='Training error')\n",
    "#     plt.xlabel('Maximum Tree Depth')\n",
    "#     plt.ylabel('Classification error')\n",
    "#     plt.title('Decision Tree with Gini Impurity and Maximum Tree Depth')\n",
    "#     plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "#     plt.show()\n",
    "\n",
    "#     print('Test error minimized at max_depth =', np.argmax(test_err), max_depth[np.argmax(test_err)])\n",
    "\n",
    "results = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('output_1.csv', 'w') as output:\n",
    "    output.write('id,target\\n')\n",
    "    for i in range(len(test_data)):\n",
    "        output.write(str(int(test_data[i][0])) + ',' + str(results[i][1]) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
